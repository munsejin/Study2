{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL004_모델저장_및_재사용.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/munsejin/Study2/blob/master/DL004_%EB%AA%A8%EB%8D%B8%EC%A0%80%EC%9E%A5_%EB%B0%8F_%EC%9E%AC%EC%82%AC%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lBEfEznLfn0",
        "colab_type": "text"
      },
      "source": [
        "## 모델 저장 및 재사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDmIvk24LXZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34c072ba-f255-4e69-9dde-aabc98f97b26"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd8ADY9WMj1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "sonar = pd.read_csv(\"/gdrive/My Drive/Colab Notebooks/인공지능강의안/data/sonar.csv\", header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG41TTw1Msxa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "dc4489ac-a31d-4ff0-b72b-2625425fb1ea"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "dataset = sonar.values\n",
        "X = dataset[:,0:60]\n",
        "Y_obj = dataset[:,60]\n",
        "\n",
        "# 레이블을 숫자로 변경\n",
        "e = LabelEncoder()\n",
        "e.fit(Y_obj)\n",
        "Y = e.transform(Y_obj)\n",
        "\n",
        "# 훈련 셋과 테스트 셋으로 분리\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
        "\n",
        "# 모델 설정\n",
        "model_sonar2 = Sequential()\n",
        "model_sonar2.add(Dense(24,  input_dim=60, activation='relu'))\n",
        "model_sonar2.add(Dense(10, activation='relu'))\n",
        "model_sonar2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model_sonar2.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델 실행\n",
        "model_sonar2.fit(X_train, Y_train, epochs=130, batch_size=5, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0716 00:19:54.131257 140494127847296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0716 00:19:54.133586 140494127847296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0716 00:19:54.146274 140494127847296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0716 00:19:54.196850 140494127847296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0716 00:19:54.384589 140494127847296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0716 00:19:54.494331 140494127847296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc713e65358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gtikzMnNUsF",
        "colab_type": "text"
      },
      "source": [
        "### 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSQGDQTNNXJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_sonar2.save(\"/gdrive/My Drive/Colab Notebooks/인공지능강의안/model/sonar_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7oG4hOfN5U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 메모리 내의 모델 삭제\n",
        "del model_sonar2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS-P7mvsODXa",
        "colab_type": "text"
      },
      "source": [
        "### 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0TtyhsdOFxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model_sonar2 = load_model(\"/gdrive/My Drive/Colab Notebooks/인공지능강의안/model/sonar_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE1xIqjbOQ07",
        "colab_type": "text"
      },
      "source": [
        "### 모델 평가하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcrTXGFBOTUz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "90cf899f-6f80-4ae3-889d-ef7bb0f207f4"
      },
      "source": [
        "model_sonar2.evaluate(X_test, Y_test)[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 946us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8095237981705439"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5eSECoIOukN",
        "colab_type": "text"
      },
      "source": [
        "## 교차검증"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvRTUR2VPKw7",
        "colab_type": "text"
      },
      "source": [
        "### 데이터를 10개로 쪼갬"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcuaNehtPAtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "n_fold = 10\n",
        "skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDxM7UNZPVdk",
        "colab_type": "text"
      },
      "source": [
        "### 10개의 데이터에 대해 각각 모델을 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66KQQkTePVxU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "586e1fc9-5306-4ff3-e4ac-4a77c7175e9f"
      },
      "source": [
        "accuracy = []\n",
        "\n",
        "for train, test in skf.split(X, Y):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=60, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X[train], Y[train], epochs=100, batch_size=5, verbose=0)\n",
        "    k_accuracy = model.evaluate(X[test], Y[test])[1]\n",
        "    accuracy.append(k_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 0s 3ms/step\n",
            "21/21 [==============================] - 0s 4ms/step\n",
            "21/21 [==============================] - 0s 6ms/step\n",
            "21/21 [==============================] - 0s 6ms/step\n",
            "21/21 [==============================] - 0s 9ms/step\n",
            "21/21 [==============================] - 0s 9ms/step\n",
            "21/21 [==============================] - 0s 10ms/step\n",
            "20/20 [==============================] - 0s 12ms/step\n",
            "20/20 [==============================] - 0s 14ms/step\n",
            "20/20 [==============================] - 0s 14ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My4JhvDHP2Wa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "e7098990-01a5-469f-b6b5-cf01c728a0dc"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8636363744735718,\n",
              " 0.761904776096344,\n",
              " 0.8095238208770752,\n",
              " 0.9047619104385376,\n",
              " 0.8095238208770752,\n",
              " 0.761904776096344,\n",
              " 0.9047619104385376,\n",
              " 0.8500000238418579,\n",
              " 0.75,\n",
              " 0.8500000238418579]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azg9nnppQJ01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72719841-41f1-4b0d-f621-5db232726ae9"
      },
      "source": [
        "## 정확도 평균\n",
        "np.average(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8266017436981201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qrJXfwHSsnN",
        "colab_type": "text"
      },
      "source": [
        "## 베스트 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEBzoV0xTHo8",
        "colab_type": "text"
      },
      "source": [
        "### 와인 데이터 셋 로드 및 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFQp_-gVSuvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "df_pre = pd.read_csv('drive/My Drive/Colab Notebooks/인공지능강의안/data/wine.csv', header=None)\n",
        "\n",
        "# sample() 함수를 사용하여 원본 데이터의 몇 %를 사용할지를 설정 (frac=1은 100%)\n",
        "# frac=0.5라면 50%만 랜덤으로 불러옴.\n",
        "df = df_pre.sample(frac=1)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:,0:12]\n",
        "Y = dataset[:,12]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTFMyiyBTX2_",
        "colab_type": "text"
      },
      "source": [
        "### 모델 설정 및 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40kc5hzfTZgL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "68351e95-ccf7-4b21-d2e0-70716df4c200"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, Y, epochs=200, batch_size=200, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0716 00:21:56.845617 140494127847296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc70c568320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkIaiMZgTrYd",
        "colab_type": "text"
      },
      "source": [
        "### 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSU3LsdtTny8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0965184a-7584-402e-911c-433f6ace90a5"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
        "model.evaluate(X_test, Y_test)\n",
        "# evaluate는 첫번째는 로스 두번째는 정확도를 반환"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1950/1950 [==============================] - 0s 207us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.042337074637030944, 0.9907692304024329]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUg_PfURUL2c",
        "colab_type": "text"
      },
      "source": [
        "## 모델 업데이트하기\n",
        "\n",
        "- epoch마다 모델의 정확도를 기록하면서 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvhWZ_0oUO8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4In43r8XKiX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델을 저장할 폴더\n",
        "MODEL_DIR = '/gdrive/My Drive/Colab Notebooks/인공지능강의안/model/'\n",
        "\n",
        "# 폴더가 없다면 폴더 생성\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.mkdir(MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIRVY8IpUl0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48c3766f-df5c-49ce-8a3e-d362aebebbe9"
      },
      "source": [
        "\n",
        "# 저장할 파일명의 정규식을 설정 (epoch의수-손실값을 파일명으로 설정)\n",
        "modelpath = MODEL_DIR + \"{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "\n",
        "# monitor=’val_loss : 모니터할 값을 설정 (val_loss : 테스트 오차, val_acc : 테스트셋 정확도, loss : 훈련 오차)\n",
        "# verbose : 진행 내용 출력 여부 (1 : 출력)\n",
        "# save_best_only=True : 이전에 저장한 모델보다 나은 경우에만 저장\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# 모델 실행 중에 콜백함수 checkpointer를 호출\n",
        "model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.06470, saving model to /gdrive/My Drive/Colab Notebooks/인공지능강의안/model/01-0.0647.hdf5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.06470 to 0.04880, saving model to /gdrive/My Drive/Colab Notebooks/인공지능강의안/model/02-0.0488.hdf5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.04880\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.04880 to 0.04517, saving model to /gdrive/My Drive/Colab Notebooks/인공지능강의안/model/04-0.0452.hdf5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.04517\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.04517 to 0.04223, saving model to /gdrive/My Drive/Colab Notebooks/인공지능강의안/model/06-0.0422.hdf5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.04223\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.04223 to 0.03714, saving model to /gdrive/My Drive/Colab Notebooks/인공지능강의안/model/08-0.0371.hdf5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.03714\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.03714\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.03714\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.03714\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.03714\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.03714\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.03714 to 0.03611, saving model to /gdrive/My Drive/Colab Notebooks/인공지능강의안/model/15-0.0361.hdf5\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.03611\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.03611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc70afb9550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su4sNNKTwSdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_pre = pd.read_csv('drive/My Drive/Colab Notebooks/인공지능강의안/data/wine.csv', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXcajDOtWAEO",
        "colab_type": "text"
      },
      "source": [
        "### 그래프로 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS973MthWFQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "# 데이터 셋 중 15%만 불러옴\n",
        "df = df_pre.sample(frac=0.15)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:,0:12]\n",
        "Y = dataset[:,12]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "MODEL_DIR = '/gdrive/My Drive/Colab Notebooks/인공지능강의안/model/'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "   os.mkdir(MODEL_DIR)\n",
        "\n",
        "modelpath = MODEL_DIR + \"{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# validation_split=0.33 : 33%만 테스트셋으로 사용\n",
        "# epochs=3500 : 긴 학습 시간 예를 들기 위해 크게 잡음\n",
        "# 실행 결과를 history에 저장\n",
        "history = model.fit(X, Y, validation_split=0.33, epochs=3500, batch_size=500, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSAHr2VhWqB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "3e17e587-5c06-4d0e-8aae-302555c783a5"
      },
      "source": [
        "# 오차값과 정확도를 불러옴\n",
        "y_vloss=history.history['val_loss']\n",
        "y_acc=history.history['acc']\n",
        "\n",
        "x_len = np.arange(len(y_acc))\n",
        "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
        "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM4ks8OGXnnQ",
        "colab_type": "text"
      },
      "source": [
        "## 자동 학습 중단 (EarlyStopping)\n",
        "\n",
        "- 테스트셋 오차가 줄지 않으면 학습을 멈추는 기능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk-KoVFaXq2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12dc1870-ab43-4e33-fb50-9fb23dee9fb6"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# patience=100 : 에포크마다 테스트 오차가 좋지 않아도 기다릴 횟수를 100으로 설정\n",
        "# 테스트 오차를 기준으로 중단 판단\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "model.fit(X, Y, validation_split=0.2, epochs=2000, batch_size=500, callbacks=[early_stopping_callback])\n",
        "\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 780 samples, validate on 195 samples\n",
            "Epoch 1/2000\n",
            "780/780 [==============================] - 1s 2ms/step - loss: 0.5489 - acc: 0.7654 - val_loss: 0.4925 - val_acc: 0.7744\n",
            "Epoch 2/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.4961 - acc: 0.7654 - val_loss: 0.4671 - val_acc: 0.7744\n",
            "Epoch 3/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.4619 - acc: 0.7641 - val_loss: 0.4397 - val_acc: 0.7744\n",
            "Epoch 4/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.4290 - acc: 0.7654 - val_loss: 0.4226 - val_acc: 0.7641\n",
            "Epoch 5/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.4109 - acc: 0.7654 - val_loss: 0.4118 - val_acc: 0.7692\n",
            "Epoch 6/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.3934 - acc: 0.7667 - val_loss: 0.3983 - val_acc: 0.7846\n",
            "Epoch 7/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.3780 - acc: 0.7756 - val_loss: 0.3882 - val_acc: 0.7846\n",
            "Epoch 8/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.3663 - acc: 0.7897 - val_loss: 0.3811 - val_acc: 0.7897\n",
            "Epoch 9/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.3581 - acc: 0.8051 - val_loss: 0.3747 - val_acc: 0.7949\n",
            "Epoch 10/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.3510 - acc: 0.8141 - val_loss: 0.3681 - val_acc: 0.8051\n",
            "Epoch 11/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.3437 - acc: 0.8308 - val_loss: 0.3613 - val_acc: 0.8205\n",
            "Epoch 12/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.3376 - acc: 0.8410 - val_loss: 0.3555 - val_acc: 0.8308\n",
            "Epoch 13/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.3315 - acc: 0.8462 - val_loss: 0.3505 - val_acc: 0.8410\n",
            "Epoch 14/2000\n",
            "780/780 [==============================] - 0s 9us/step - loss: 0.3255 - acc: 0.8564 - val_loss: 0.3457 - val_acc: 0.8410\n",
            "Epoch 15/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.3194 - acc: 0.8577 - val_loss: 0.3403 - val_acc: 0.8462\n",
            "Epoch 16/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.3138 - acc: 0.8628 - val_loss: 0.3349 - val_acc: 0.8513\n",
            "Epoch 17/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.3081 - acc: 0.8731 - val_loss: 0.3301 - val_acc: 0.8513\n",
            "Epoch 18/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.3032 - acc: 0.8795 - val_loss: 0.3259 - val_acc: 0.8513\n",
            "Epoch 19/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.2969 - acc: 0.8808 - val_loss: 0.3238 - val_acc: 0.8513\n",
            "Epoch 20/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.2937 - acc: 0.8808 - val_loss: 0.3201 - val_acc: 0.8513\n",
            "Epoch 21/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.2885 - acc: 0.8833 - val_loss: 0.3134 - val_acc: 0.8615\n",
            "Epoch 22/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.2827 - acc: 0.8962 - val_loss: 0.3088 - val_acc: 0.8718\n",
            "Epoch 23/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.2785 - acc: 0.9013 - val_loss: 0.3047 - val_acc: 0.8769\n",
            "Epoch 24/2000\n",
            "780/780 [==============================] - 0s 70us/step - loss: 0.2729 - acc: 0.9026 - val_loss: 0.3013 - val_acc: 0.8769\n",
            "Epoch 25/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.2680 - acc: 0.9038 - val_loss: 0.2986 - val_acc: 0.8769\n",
            "Epoch 26/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.2636 - acc: 0.9064 - val_loss: 0.2940 - val_acc: 0.8821\n",
            "Epoch 27/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.2581 - acc: 0.9115 - val_loss: 0.2885 - val_acc: 0.8821\n",
            "Epoch 28/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.2530 - acc: 0.9154 - val_loss: 0.2842 - val_acc: 0.8872\n",
            "Epoch 29/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.2489 - acc: 0.9179 - val_loss: 0.2797 - val_acc: 0.8923\n",
            "Epoch 30/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.2449 - acc: 0.9205 - val_loss: 0.2768 - val_acc: 0.8923\n",
            "Epoch 31/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.2402 - acc: 0.9179 - val_loss: 0.2724 - val_acc: 0.8974\n",
            "Epoch 32/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.2361 - acc: 0.9231 - val_loss: 0.2679 - val_acc: 0.9077\n",
            "Epoch 33/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.2328 - acc: 0.9256 - val_loss: 0.2641 - val_acc: 0.9026\n",
            "Epoch 34/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.2302 - acc: 0.9282 - val_loss: 0.2607 - val_acc: 0.9077\n",
            "Epoch 35/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.2277 - acc: 0.9295 - val_loss: 0.2591 - val_acc: 0.9077\n",
            "Epoch 36/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.2247 - acc: 0.9269 - val_loss: 0.2553 - val_acc: 0.9026\n",
            "Epoch 37/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.2218 - acc: 0.9295 - val_loss: 0.2521 - val_acc: 0.9128\n",
            "Epoch 38/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.2196 - acc: 0.9308 - val_loss: 0.2497 - val_acc: 0.9231\n",
            "Epoch 39/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.2177 - acc: 0.9308 - val_loss: 0.2475 - val_acc: 0.9231\n",
            "Epoch 40/2000\n",
            "780/780 [==============================] - 0s 29us/step - loss: 0.2158 - acc: 0.9308 - val_loss: 0.2456 - val_acc: 0.9231\n",
            "Epoch 41/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.2143 - acc: 0.9308 - val_loss: 0.2445 - val_acc: 0.9179\n",
            "Epoch 42/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.2123 - acc: 0.9308 - val_loss: 0.2421 - val_acc: 0.9231\n",
            "Epoch 43/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.2106 - acc: 0.9295 - val_loss: 0.2404 - val_acc: 0.9282\n",
            "Epoch 44/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.2095 - acc: 0.9295 - val_loss: 0.2386 - val_acc: 0.9282\n",
            "Epoch 45/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.2077 - acc: 0.9295 - val_loss: 0.2380 - val_acc: 0.9282\n",
            "Epoch 46/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.2066 - acc: 0.9295 - val_loss: 0.2379 - val_acc: 0.9231\n",
            "Epoch 47/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.2059 - acc: 0.9295 - val_loss: 0.2352 - val_acc: 0.9282\n",
            "Epoch 48/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.2044 - acc: 0.9295 - val_loss: 0.2326 - val_acc: 0.9282\n",
            "Epoch 49/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.2035 - acc: 0.9321 - val_loss: 0.2311 - val_acc: 0.9333\n",
            "Epoch 50/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.2030 - acc: 0.9346 - val_loss: 0.2305 - val_acc: 0.9333\n",
            "Epoch 51/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.2012 - acc: 0.9308 - val_loss: 0.2285 - val_acc: 0.9333\n",
            "Epoch 52/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.2001 - acc: 0.9333 - val_loss: 0.2271 - val_acc: 0.9333\n",
            "Epoch 53/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1993 - acc: 0.9321 - val_loss: 0.2257 - val_acc: 0.9333\n",
            "Epoch 54/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1982 - acc: 0.9333 - val_loss: 0.2250 - val_acc: 0.9333\n",
            "Epoch 55/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1974 - acc: 0.9333 - val_loss: 0.2240 - val_acc: 0.9333\n",
            "Epoch 56/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.1963 - acc: 0.9346 - val_loss: 0.2222 - val_acc: 0.9333\n",
            "Epoch 57/2000\n",
            "780/780 [==============================] - 0s 24us/step - loss: 0.1957 - acc: 0.9333 - val_loss: 0.2208 - val_acc: 0.9333\n",
            "Epoch 58/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1946 - acc: 0.9346 - val_loss: 0.2196 - val_acc: 0.9333\n",
            "Epoch 59/2000\n",
            "780/780 [==============================] - 0s 22us/step - loss: 0.1941 - acc: 0.9359 - val_loss: 0.2185 - val_acc: 0.9333\n",
            "Epoch 60/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1926 - acc: 0.9346 - val_loss: 0.2188 - val_acc: 0.9333\n",
            "Epoch 61/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1922 - acc: 0.9346 - val_loss: 0.2193 - val_acc: 0.9282\n",
            "Epoch 62/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.1917 - acc: 0.9333 - val_loss: 0.2161 - val_acc: 0.9333\n",
            "Epoch 63/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1904 - acc: 0.9359 - val_loss: 0.2138 - val_acc: 0.9333\n",
            "Epoch 64/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.1903 - acc: 0.9346 - val_loss: 0.2127 - val_acc: 0.9333\n",
            "Epoch 65/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1892 - acc: 0.9333 - val_loss: 0.2134 - val_acc: 0.9333\n",
            "Epoch 66/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.1884 - acc: 0.9359 - val_loss: 0.2124 - val_acc: 0.9333\n",
            "Epoch 67/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1869 - acc: 0.9359 - val_loss: 0.2091 - val_acc: 0.9385\n",
            "Epoch 68/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.1859 - acc: 0.9346 - val_loss: 0.2078 - val_acc: 0.9385\n",
            "Epoch 69/2000\n",
            "780/780 [==============================] - 0s 26us/step - loss: 0.1862 - acc: 0.9385 - val_loss: 0.2061 - val_acc: 0.9385\n",
            "Epoch 70/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1840 - acc: 0.9359 - val_loss: 0.2063 - val_acc: 0.9385\n",
            "Epoch 71/2000\n",
            "780/780 [==============================] - 0s 22us/step - loss: 0.1842 - acc: 0.9359 - val_loss: 0.2089 - val_acc: 0.9333\n",
            "Epoch 72/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1839 - acc: 0.9359 - val_loss: 0.2035 - val_acc: 0.9385\n",
            "Epoch 73/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1819 - acc: 0.9359 - val_loss: 0.2006 - val_acc: 0.9385\n",
            "Epoch 74/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1813 - acc: 0.9346 - val_loss: 0.1995 - val_acc: 0.9385\n",
            "Epoch 75/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1802 - acc: 0.9333 - val_loss: 0.1996 - val_acc: 0.9385\n",
            "Epoch 76/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1791 - acc: 0.9359 - val_loss: 0.1996 - val_acc: 0.9385\n",
            "Epoch 77/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1789 - acc: 0.9359 - val_loss: 0.1983 - val_acc: 0.9385\n",
            "Epoch 78/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1779 - acc: 0.9359 - val_loss: 0.1967 - val_acc: 0.9385\n",
            "Epoch 79/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1768 - acc: 0.9359 - val_loss: 0.1947 - val_acc: 0.9385\n",
            "Epoch 80/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1765 - acc: 0.9346 - val_loss: 0.1934 - val_acc: 0.9385\n",
            "Epoch 81/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1758 - acc: 0.9346 - val_loss: 0.1932 - val_acc: 0.9385\n",
            "Epoch 82/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1751 - acc: 0.9359 - val_loss: 0.1946 - val_acc: 0.9385\n",
            "Epoch 83/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1746 - acc: 0.9372 - val_loss: 0.1921 - val_acc: 0.9385\n",
            "Epoch 84/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1737 - acc: 0.9346 - val_loss: 0.1891 - val_acc: 0.9385\n",
            "Epoch 85/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1730 - acc: 0.9359 - val_loss: 0.1882 - val_acc: 0.9385\n",
            "Epoch 86/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1723 - acc: 0.9359 - val_loss: 0.1879 - val_acc: 0.9385\n",
            "Epoch 87/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1714 - acc: 0.9346 - val_loss: 0.1890 - val_acc: 0.9385\n",
            "Epoch 88/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1712 - acc: 0.9359 - val_loss: 0.1881 - val_acc: 0.9385\n",
            "Epoch 89/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1704 - acc: 0.9385 - val_loss: 0.1867 - val_acc: 0.9385\n",
            "Epoch 90/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1695 - acc: 0.9372 - val_loss: 0.1842 - val_acc: 0.9385\n",
            "Epoch 91/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1691 - acc: 0.9346 - val_loss: 0.1829 - val_acc: 0.9385\n",
            "Epoch 92/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1686 - acc: 0.9359 - val_loss: 0.1832 - val_acc: 0.9385\n",
            "Epoch 93/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1676 - acc: 0.9346 - val_loss: 0.1837 - val_acc: 0.9385\n",
            "Epoch 94/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1672 - acc: 0.9385 - val_loss: 0.1835 - val_acc: 0.9385\n",
            "Epoch 95/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1669 - acc: 0.9397 - val_loss: 0.1817 - val_acc: 0.9385\n",
            "Epoch 96/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1660 - acc: 0.9372 - val_loss: 0.1807 - val_acc: 0.9385\n",
            "Epoch 97/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1653 - acc: 0.9372 - val_loss: 0.1802 - val_acc: 0.9385\n",
            "Epoch 98/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1650 - acc: 0.9385 - val_loss: 0.1791 - val_acc: 0.9385\n",
            "Epoch 99/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1641 - acc: 0.9385 - val_loss: 0.1788 - val_acc: 0.9385\n",
            "Epoch 100/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1636 - acc: 0.9385 - val_loss: 0.1774 - val_acc: 0.9385\n",
            "Epoch 101/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1634 - acc: 0.9385 - val_loss: 0.1759 - val_acc: 0.9385\n",
            "Epoch 102/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1624 - acc: 0.9385 - val_loss: 0.1742 - val_acc: 0.9436\n",
            "Epoch 103/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1628 - acc: 0.9333 - val_loss: 0.1734 - val_acc: 0.9436\n",
            "Epoch 104/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1613 - acc: 0.9372 - val_loss: 0.1763 - val_acc: 0.9385\n",
            "Epoch 105/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1608 - acc: 0.9410 - val_loss: 0.1799 - val_acc: 0.9385\n",
            "Epoch 106/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1628 - acc: 0.9385 - val_loss: 0.1771 - val_acc: 0.9385\n",
            "Epoch 107/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1596 - acc: 0.9410 - val_loss: 0.1699 - val_acc: 0.9436\n",
            "Epoch 108/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1610 - acc: 0.9397 - val_loss: 0.1691 - val_acc: 0.9436\n",
            "Epoch 109/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1605 - acc: 0.9359 - val_loss: 0.1719 - val_acc: 0.9385\n",
            "Epoch 110/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1615 - acc: 0.9385 - val_loss: 0.1797 - val_acc: 0.9333\n",
            "Epoch 111/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1602 - acc: 0.9397 - val_loss: 0.1707 - val_acc: 0.9385\n",
            "Epoch 112/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1565 - acc: 0.9385 - val_loss: 0.1665 - val_acc: 0.9436\n",
            "Epoch 113/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1584 - acc: 0.9372 - val_loss: 0.1659 - val_acc: 0.9436\n",
            "Epoch 114/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1573 - acc: 0.9359 - val_loss: 0.1687 - val_acc: 0.9385\n",
            "Epoch 115/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1578 - acc: 0.9385 - val_loss: 0.1720 - val_acc: 0.9385\n",
            "Epoch 116/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1569 - acc: 0.9397 - val_loss: 0.1662 - val_acc: 0.9436\n",
            "Epoch 117/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.1546 - acc: 0.9372 - val_loss: 0.1646 - val_acc: 0.9436\n",
            "Epoch 118/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1551 - acc: 0.9385 - val_loss: 0.1643 - val_acc: 0.9436\n",
            "Epoch 119/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1534 - acc: 0.9397 - val_loss: 0.1681 - val_acc: 0.9436\n",
            "Epoch 120/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1537 - acc: 0.9410 - val_loss: 0.1685 - val_acc: 0.9436\n",
            "Epoch 121/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1539 - acc: 0.9410 - val_loss: 0.1638 - val_acc: 0.9487\n",
            "Epoch 122/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1529 - acc: 0.9397 - val_loss: 0.1603 - val_acc: 0.9436\n",
            "Epoch 123/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1539 - acc: 0.9359 - val_loss: 0.1609 - val_acc: 0.9487\n",
            "Epoch 124/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1511 - acc: 0.9397 - val_loss: 0.1677 - val_acc: 0.9436\n",
            "Epoch 125/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1521 - acc: 0.9385 - val_loss: 0.1687 - val_acc: 0.9436\n",
            "Epoch 126/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.1518 - acc: 0.9385 - val_loss: 0.1619 - val_acc: 0.9487\n",
            "Epoch 127/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1513 - acc: 0.9372 - val_loss: 0.1578 - val_acc: 0.9487\n",
            "Epoch 128/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1505 - acc: 0.9397 - val_loss: 0.1590 - val_acc: 0.9487\n",
            "Epoch 129/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1487 - acc: 0.9410 - val_loss: 0.1629 - val_acc: 0.9487\n",
            "Epoch 130/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1490 - acc: 0.9410 - val_loss: 0.1647 - val_acc: 0.9436\n",
            "Epoch 131/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1493 - acc: 0.9385 - val_loss: 0.1609 - val_acc: 0.9487\n",
            "Epoch 132/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1475 - acc: 0.9385 - val_loss: 0.1558 - val_acc: 0.9487\n",
            "Epoch 133/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1484 - acc: 0.9359 - val_loss: 0.1551 - val_acc: 0.9487\n",
            "Epoch 134/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1481 - acc: 0.9346 - val_loss: 0.1587 - val_acc: 0.9487\n",
            "Epoch 135/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1469 - acc: 0.9385 - val_loss: 0.1585 - val_acc: 0.9487\n",
            "Epoch 136/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1458 - acc: 0.9385 - val_loss: 0.1544 - val_acc: 0.9487\n",
            "Epoch 137/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1459 - acc: 0.9410 - val_loss: 0.1532 - val_acc: 0.9487\n",
            "Epoch 138/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1456 - acc: 0.9397 - val_loss: 0.1557 - val_acc: 0.9487\n",
            "Epoch 139/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1453 - acc: 0.9385 - val_loss: 0.1561 - val_acc: 0.9487\n",
            "Epoch 140/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1439 - acc: 0.9385 - val_loss: 0.1522 - val_acc: 0.9487\n",
            "Epoch 141/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1441 - acc: 0.9397 - val_loss: 0.1511 - val_acc: 0.9487\n",
            "Epoch 142/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1438 - acc: 0.9385 - val_loss: 0.1543 - val_acc: 0.9487\n",
            "Epoch 143/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.1431 - acc: 0.9385 - val_loss: 0.1606 - val_acc: 0.9436\n",
            "Epoch 144/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1443 - acc: 0.9410 - val_loss: 0.1550 - val_acc: 0.9487\n",
            "Epoch 145/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.1423 - acc: 0.9397 - val_loss: 0.1491 - val_acc: 0.9487\n",
            "Epoch 146/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1423 - acc: 0.9397 - val_loss: 0.1487 - val_acc: 0.9487\n",
            "Epoch 147/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1416 - acc: 0.9397 - val_loss: 0.1508 - val_acc: 0.9487\n",
            "Epoch 148/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1406 - acc: 0.9410 - val_loss: 0.1535 - val_acc: 0.9487\n",
            "Epoch 149/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1406 - acc: 0.9372 - val_loss: 0.1517 - val_acc: 0.9487\n",
            "Epoch 150/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.1402 - acc: 0.9410 - val_loss: 0.1490 - val_acc: 0.9487\n",
            "Epoch 151/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.1396 - acc: 0.9410 - val_loss: 0.1488 - val_acc: 0.9487\n",
            "Epoch 152/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.1390 - acc: 0.9410 - val_loss: 0.1504 - val_acc: 0.9487\n",
            "Epoch 153/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.1388 - acc: 0.9410 - val_loss: 0.1490 - val_acc: 0.9487\n",
            "Epoch 154/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.1382 - acc: 0.9410 - val_loss: 0.1460 - val_acc: 0.9487\n",
            "Epoch 155/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1380 - acc: 0.9410 - val_loss: 0.1441 - val_acc: 0.9487\n",
            "Epoch 156/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1389 - acc: 0.9372 - val_loss: 0.1445 - val_acc: 0.9487\n",
            "Epoch 157/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1377 - acc: 0.9423 - val_loss: 0.1500 - val_acc: 0.9487\n",
            "Epoch 158/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.1372 - acc: 0.9397 - val_loss: 0.1487 - val_acc: 0.9487\n",
            "Epoch 159/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.1365 - acc: 0.9423 - val_loss: 0.1445 - val_acc: 0.9487\n",
            "Epoch 160/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1358 - acc: 0.9423 - val_loss: 0.1432 - val_acc: 0.9487\n",
            "Epoch 161/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.1355 - acc: 0.9423 - val_loss: 0.1439 - val_acc: 0.9487\n",
            "Epoch 162/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1352 - acc: 0.9410 - val_loss: 0.1448 - val_acc: 0.9487\n",
            "Epoch 163/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.1348 - acc: 0.9423 - val_loss: 0.1426 - val_acc: 0.9487\n",
            "Epoch 164/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.1342 - acc: 0.9410 - val_loss: 0.1406 - val_acc: 0.9487\n",
            "Epoch 165/2000\n",
            "780/780 [==============================] - 0s 22us/step - loss: 0.1344 - acc: 0.9410 - val_loss: 0.1429 - val_acc: 0.9487\n",
            "Epoch 166/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.1330 - acc: 0.9423 - val_loss: 0.1481 - val_acc: 0.9487\n",
            "Epoch 167/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1343 - acc: 0.9423 - val_loss: 0.1457 - val_acc: 0.9487\n",
            "Epoch 168/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1326 - acc: 0.9410 - val_loss: 0.1392 - val_acc: 0.9487\n",
            "Epoch 169/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1334 - acc: 0.9410 - val_loss: 0.1383 - val_acc: 0.9487\n",
            "Epoch 170/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1325 - acc: 0.9410 - val_loss: 0.1434 - val_acc: 0.9487\n",
            "Epoch 171/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1318 - acc: 0.9449 - val_loss: 0.1450 - val_acc: 0.9487\n",
            "Epoch 172/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1321 - acc: 0.9449 - val_loss: 0.1403 - val_acc: 0.9487\n",
            "Epoch 173/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1314 - acc: 0.9410 - val_loss: 0.1389 - val_acc: 0.9487\n",
            "Epoch 174/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1303 - acc: 0.9410 - val_loss: 0.1416 - val_acc: 0.9487\n",
            "Epoch 175/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1306 - acc: 0.9462 - val_loss: 0.1390 - val_acc: 0.9487\n",
            "Epoch 176/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1291 - acc: 0.9423 - val_loss: 0.1353 - val_acc: 0.9487\n",
            "Epoch 177/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1308 - acc: 0.9410 - val_loss: 0.1356 - val_acc: 0.9487\n",
            "Epoch 178/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.1292 - acc: 0.9423 - val_loss: 0.1377 - val_acc: 0.9487\n",
            "Epoch 179/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1284 - acc: 0.9449 - val_loss: 0.1418 - val_acc: 0.9487\n",
            "Epoch 180/2000\n",
            "780/780 [==============================] - 0s 22us/step - loss: 0.1288 - acc: 0.9436 - val_loss: 0.1378 - val_acc: 0.9487\n",
            "Epoch 181/2000\n",
            "780/780 [==============================] - 0s 22us/step - loss: 0.1271 - acc: 0.9423 - val_loss: 0.1334 - val_acc: 0.9487\n",
            "Epoch 182/2000\n",
            "780/780 [==============================] - 0s 25us/step - loss: 0.1291 - acc: 0.9397 - val_loss: 0.1339 - val_acc: 0.9487\n",
            "Epoch 183/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1264 - acc: 0.9436 - val_loss: 0.1427 - val_acc: 0.9487\n",
            "Epoch 184/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1289 - acc: 0.9423 - val_loss: 0.1446 - val_acc: 0.9487\n",
            "Epoch 185/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1287 - acc: 0.9397 - val_loss: 0.1335 - val_acc: 0.9487\n",
            "Epoch 186/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1282 - acc: 0.9410 - val_loss: 0.1325 - val_acc: 0.9487\n",
            "Epoch 187/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1248 - acc: 0.9423 - val_loss: 0.1420 - val_acc: 0.9487\n",
            "Epoch 188/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1277 - acc: 0.9423 - val_loss: 0.1462 - val_acc: 0.9436\n",
            "Epoch 189/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1274 - acc: 0.9436 - val_loss: 0.1329 - val_acc: 0.9487\n",
            "Epoch 190/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.1266 - acc: 0.9410 - val_loss: 0.1300 - val_acc: 0.9487\n",
            "Epoch 191/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1258 - acc: 0.9449 - val_loss: 0.1364 - val_acc: 0.9487\n",
            "Epoch 192/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1273 - acc: 0.9449 - val_loss: 0.1423 - val_acc: 0.9436\n",
            "Epoch 193/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1246 - acc: 0.9423 - val_loss: 0.1308 - val_acc: 0.9487\n",
            "Epoch 194/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1268 - acc: 0.9423 - val_loss: 0.1295 - val_acc: 0.9487\n",
            "Epoch 195/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1229 - acc: 0.9462 - val_loss: 0.1409 - val_acc: 0.9436\n",
            "Epoch 196/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.1250 - acc: 0.9436 - val_loss: 0.1438 - val_acc: 0.9436\n",
            "Epoch 197/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1237 - acc: 0.9436 - val_loss: 0.1301 - val_acc: 0.9487\n",
            "Epoch 198/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1244 - acc: 0.9449 - val_loss: 0.1276 - val_acc: 0.9487\n",
            "Epoch 199/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1225 - acc: 0.9462 - val_loss: 0.1366 - val_acc: 0.9487\n",
            "Epoch 200/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1237 - acc: 0.9436 - val_loss: 0.1466 - val_acc: 0.9385\n",
            "Epoch 201/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1247 - acc: 0.9436 - val_loss: 0.1312 - val_acc: 0.9487\n",
            "Epoch 202/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1207 - acc: 0.9449 - val_loss: 0.1264 - val_acc: 0.9487\n",
            "Epoch 203/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1232 - acc: 0.9513 - val_loss: 0.1310 - val_acc: 0.9487\n",
            "Epoch 204/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1214 - acc: 0.9436 - val_loss: 0.1454 - val_acc: 0.9385\n",
            "Epoch 205/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1237 - acc: 0.9423 - val_loss: 0.1336 - val_acc: 0.9487\n",
            "Epoch 206/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1199 - acc: 0.9410 - val_loss: 0.1257 - val_acc: 0.9487\n",
            "Epoch 207/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1205 - acc: 0.9474 - val_loss: 0.1274 - val_acc: 0.9487\n",
            "Epoch 208/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1174 - acc: 0.9436 - val_loss: 0.1361 - val_acc: 0.9487\n",
            "Epoch 209/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1207 - acc: 0.9436 - val_loss: 0.1383 - val_acc: 0.9436\n",
            "Epoch 210/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1198 - acc: 0.9423 - val_loss: 0.1264 - val_acc: 0.9487\n",
            "Epoch 211/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1182 - acc: 0.9462 - val_loss: 0.1253 - val_acc: 0.9487\n",
            "Epoch 212/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1179 - acc: 0.9423 - val_loss: 0.1305 - val_acc: 0.9487\n",
            "Epoch 213/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1172 - acc: 0.9449 - val_loss: 0.1308 - val_acc: 0.9487\n",
            "Epoch 214/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1160 - acc: 0.9436 - val_loss: 0.1250 - val_acc: 0.9487\n",
            "Epoch 215/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1162 - acc: 0.9449 - val_loss: 0.1237 - val_acc: 0.9487\n",
            "Epoch 216/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.1165 - acc: 0.9474 - val_loss: 0.1253 - val_acc: 0.9487\n",
            "Epoch 217/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1154 - acc: 0.9436 - val_loss: 0.1282 - val_acc: 0.9487\n",
            "Epoch 218/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1149 - acc: 0.9462 - val_loss: 0.1269 - val_acc: 0.9487\n",
            "Epoch 219/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.1144 - acc: 0.9436 - val_loss: 0.1249 - val_acc: 0.9487\n",
            "Epoch 220/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1143 - acc: 0.9436 - val_loss: 0.1246 - val_acc: 0.9487\n",
            "Epoch 221/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1140 - acc: 0.9436 - val_loss: 0.1248 - val_acc: 0.9487\n",
            "Epoch 222/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1135 - acc: 0.9436 - val_loss: 0.1270 - val_acc: 0.9487\n",
            "Epoch 223/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1133 - acc: 0.9462 - val_loss: 0.1255 - val_acc: 0.9487\n",
            "Epoch 224/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1139 - acc: 0.9436 - val_loss: 0.1241 - val_acc: 0.9487\n",
            "Epoch 225/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1124 - acc: 0.9449 - val_loss: 0.1278 - val_acc: 0.9487\n",
            "Epoch 226/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1126 - acc: 0.9436 - val_loss: 0.1258 - val_acc: 0.9487\n",
            "Epoch 227/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.1119 - acc: 0.9462 - val_loss: 0.1220 - val_acc: 0.9487\n",
            "Epoch 228/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1125 - acc: 0.9474 - val_loss: 0.1230 - val_acc: 0.9487\n",
            "Epoch 229/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1136 - acc: 0.9474 - val_loss: 0.1279 - val_acc: 0.9487\n",
            "Epoch 230/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1112 - acc: 0.9462 - val_loss: 0.1206 - val_acc: 0.9487\n",
            "Epoch 231/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1112 - acc: 0.9500 - val_loss: 0.1194 - val_acc: 0.9487\n",
            "Epoch 232/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1108 - acc: 0.9526 - val_loss: 0.1250 - val_acc: 0.9487\n",
            "Epoch 233/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1143 - acc: 0.9462 - val_loss: 0.1288 - val_acc: 0.9487\n",
            "Epoch 234/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1113 - acc: 0.9449 - val_loss: 0.1186 - val_acc: 0.9487\n",
            "Epoch 235/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1115 - acc: 0.9526 - val_loss: 0.1198 - val_acc: 0.9487\n",
            "Epoch 236/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1107 - acc: 0.9474 - val_loss: 0.1276 - val_acc: 0.9487\n",
            "Epoch 237/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1099 - acc: 0.9449 - val_loss: 0.1226 - val_acc: 0.9487\n",
            "Epoch 238/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1086 - acc: 0.9487 - val_loss: 0.1179 - val_acc: 0.9487\n",
            "Epoch 239/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1092 - acc: 0.9526 - val_loss: 0.1191 - val_acc: 0.9487\n",
            "Epoch 240/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1079 - acc: 0.9462 - val_loss: 0.1207 - val_acc: 0.9487\n",
            "Epoch 241/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1087 - acc: 0.9462 - val_loss: 0.1192 - val_acc: 0.9487\n",
            "Epoch 242/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.1070 - acc: 0.9487 - val_loss: 0.1156 - val_acc: 0.9487\n",
            "Epoch 243/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1084 - acc: 0.9526 - val_loss: 0.1178 - val_acc: 0.9487\n",
            "Epoch 244/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1079 - acc: 0.9462 - val_loss: 0.1229 - val_acc: 0.9487\n",
            "Epoch 245/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1068 - acc: 0.9474 - val_loss: 0.1179 - val_acc: 0.9487\n",
            "Epoch 246/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.1060 - acc: 0.9487 - val_loss: 0.1165 - val_acc: 0.9487\n",
            "Epoch 247/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1059 - acc: 0.9526 - val_loss: 0.1188 - val_acc: 0.9487\n",
            "Epoch 248/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1058 - acc: 0.9487 - val_loss: 0.1205 - val_acc: 0.9487\n",
            "Epoch 249/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.1057 - acc: 0.9474 - val_loss: 0.1156 - val_acc: 0.9487\n",
            "Epoch 250/2000\n",
            "780/780 [==============================] - 0s 25us/step - loss: 0.1062 - acc: 0.9564 - val_loss: 0.1135 - val_acc: 0.9538\n",
            "Epoch 251/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.1052 - acc: 0.9526 - val_loss: 0.1209 - val_acc: 0.9487\n",
            "Epoch 252/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1058 - acc: 0.9449 - val_loss: 0.1243 - val_acc: 0.9487\n",
            "Epoch 253/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1058 - acc: 0.9487 - val_loss: 0.1145 - val_acc: 0.9487\n",
            "Epoch 254/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1040 - acc: 0.9526 - val_loss: 0.1131 - val_acc: 0.9487\n",
            "Epoch 255/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.1039 - acc: 0.9513 - val_loss: 0.1170 - val_acc: 0.9487\n",
            "Epoch 256/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1032 - acc: 0.9500 - val_loss: 0.1201 - val_acc: 0.9487\n",
            "Epoch 257/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.1041 - acc: 0.9474 - val_loss: 0.1183 - val_acc: 0.9487\n",
            "Epoch 258/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1055 - acc: 0.9500 - val_loss: 0.1129 - val_acc: 0.9487\n",
            "Epoch 259/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1024 - acc: 0.9526 - val_loss: 0.1196 - val_acc: 0.9487\n",
            "Epoch 260/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1039 - acc: 0.9487 - val_loss: 0.1232 - val_acc: 0.9487\n",
            "Epoch 261/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.1034 - acc: 0.9462 - val_loss: 0.1125 - val_acc: 0.9487\n",
            "Epoch 262/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1018 - acc: 0.9526 - val_loss: 0.1114 - val_acc: 0.9538\n",
            "Epoch 263/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.1016 - acc: 0.9513 - val_loss: 0.1163 - val_acc: 0.9487\n",
            "Epoch 264/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.1008 - acc: 0.9500 - val_loss: 0.1209 - val_acc: 0.9538\n",
            "Epoch 265/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1032 - acc: 0.9449 - val_loss: 0.1164 - val_acc: 0.9487\n",
            "Epoch 266/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0993 - acc: 0.9500 - val_loss: 0.1087 - val_acc: 0.9538\n",
            "Epoch 267/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.1042 - acc: 0.9615 - val_loss: 0.1099 - val_acc: 0.9538\n",
            "Epoch 268/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.1009 - acc: 0.9487 - val_loss: 0.1233 - val_acc: 0.9487\n",
            "Epoch 269/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.1025 - acc: 0.9449 - val_loss: 0.1179 - val_acc: 0.9487\n",
            "Epoch 270/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.0996 - acc: 0.9513 - val_loss: 0.1087 - val_acc: 0.9538\n",
            "Epoch 271/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.1004 - acc: 0.9538 - val_loss: 0.1089 - val_acc: 0.9538\n",
            "Epoch 272/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0997 - acc: 0.9526 - val_loss: 0.1163 - val_acc: 0.9487\n",
            "Epoch 273/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0991 - acc: 0.9500 - val_loss: 0.1137 - val_acc: 0.9487\n",
            "Epoch 274/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0993 - acc: 0.9513 - val_loss: 0.1095 - val_acc: 0.9487\n",
            "Epoch 275/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0985 - acc: 0.9526 - val_loss: 0.1117 - val_acc: 0.9487\n",
            "Epoch 276/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0975 - acc: 0.9526 - val_loss: 0.1101 - val_acc: 0.9487\n",
            "Epoch 277/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0972 - acc: 0.9538 - val_loss: 0.1104 - val_acc: 0.9487\n",
            "Epoch 278/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0971 - acc: 0.9526 - val_loss: 0.1099 - val_acc: 0.9487\n",
            "Epoch 279/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0966 - acc: 0.9551 - val_loss: 0.1080 - val_acc: 0.9538\n",
            "Epoch 280/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0965 - acc: 0.9564 - val_loss: 0.1086 - val_acc: 0.9487\n",
            "Epoch 281/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0961 - acc: 0.9590 - val_loss: 0.1109 - val_acc: 0.9487\n",
            "Epoch 282/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0961 - acc: 0.9538 - val_loss: 0.1124 - val_acc: 0.9487\n",
            "Epoch 283/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0961 - acc: 0.9538 - val_loss: 0.1113 - val_acc: 0.9487\n",
            "Epoch 284/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0959 - acc: 0.9526 - val_loss: 0.1083 - val_acc: 0.9538\n",
            "Epoch 285/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0955 - acc: 0.9551 - val_loss: 0.1095 - val_acc: 0.9487\n",
            "Epoch 286/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0954 - acc: 0.9538 - val_loss: 0.1111 - val_acc: 0.9487\n",
            "Epoch 287/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0952 - acc: 0.9538 - val_loss: 0.1061 - val_acc: 0.9538\n",
            "Epoch 288/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0945 - acc: 0.9564 - val_loss: 0.1060 - val_acc: 0.9538\n",
            "Epoch 289/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0941 - acc: 0.9564 - val_loss: 0.1066 - val_acc: 0.9538\n",
            "Epoch 290/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0938 - acc: 0.9590 - val_loss: 0.1079 - val_acc: 0.9487\n",
            "Epoch 291/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0940 - acc: 0.9564 - val_loss: 0.1084 - val_acc: 0.9487\n",
            "Epoch 292/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0932 - acc: 0.9551 - val_loss: 0.1043 - val_acc: 0.9538\n",
            "Epoch 293/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0937 - acc: 0.9590 - val_loss: 0.1053 - val_acc: 0.9538\n",
            "Epoch 294/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0937 - acc: 0.9538 - val_loss: 0.1080 - val_acc: 0.9487\n",
            "Epoch 295/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0926 - acc: 0.9577 - val_loss: 0.1048 - val_acc: 0.9538\n",
            "Epoch 296/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0936 - acc: 0.9577 - val_loss: 0.1059 - val_acc: 0.9487\n",
            "Epoch 297/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0918 - acc: 0.9577 - val_loss: 0.1125 - val_acc: 0.9538\n",
            "Epoch 298/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0937 - acc: 0.9513 - val_loss: 0.1066 - val_acc: 0.9538\n",
            "Epoch 299/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0917 - acc: 0.9615 - val_loss: 0.1003 - val_acc: 0.9538\n",
            "Epoch 300/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0930 - acc: 0.9641 - val_loss: 0.1036 - val_acc: 0.9538\n",
            "Epoch 301/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0910 - acc: 0.9603 - val_loss: 0.1092 - val_acc: 0.9538\n",
            "Epoch 302/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0917 - acc: 0.9538 - val_loss: 0.1045 - val_acc: 0.9590\n",
            "Epoch 303/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0912 - acc: 0.9615 - val_loss: 0.1004 - val_acc: 0.9538\n",
            "Epoch 304/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0911 - acc: 0.9641 - val_loss: 0.1068 - val_acc: 0.9538\n",
            "Epoch 305/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0945 - acc: 0.9526 - val_loss: 0.1099 - val_acc: 0.9538\n",
            "Epoch 306/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0937 - acc: 0.9538 - val_loss: 0.0992 - val_acc: 0.9538\n",
            "Epoch 307/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0909 - acc: 0.9654 - val_loss: 0.1045 - val_acc: 0.9590\n",
            "Epoch 308/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0893 - acc: 0.9577 - val_loss: 0.1114 - val_acc: 0.9487\n",
            "Epoch 309/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0914 - acc: 0.9551 - val_loss: 0.1038 - val_acc: 0.9590\n",
            "Epoch 310/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0888 - acc: 0.9615 - val_loss: 0.0987 - val_acc: 0.9538\n",
            "Epoch 311/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0896 - acc: 0.9654 - val_loss: 0.0998 - val_acc: 0.9538\n",
            "Epoch 312/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0887 - acc: 0.9603 - val_loss: 0.1051 - val_acc: 0.9538\n",
            "Epoch 313/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0885 - acc: 0.9590 - val_loss: 0.1036 - val_acc: 0.9590\n",
            "Epoch 314/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0880 - acc: 0.9603 - val_loss: 0.1001 - val_acc: 0.9538\n",
            "Epoch 315/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0880 - acc: 0.9679 - val_loss: 0.1011 - val_acc: 0.9590\n",
            "Epoch 316/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0876 - acc: 0.9654 - val_loss: 0.1034 - val_acc: 0.9590\n",
            "Epoch 317/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0874 - acc: 0.9603 - val_loss: 0.1052 - val_acc: 0.9590\n",
            "Epoch 318/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0888 - acc: 0.9590 - val_loss: 0.1012 - val_acc: 0.9590\n",
            "Epoch 319/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0881 - acc: 0.9590 - val_loss: 0.1012 - val_acc: 0.9590\n",
            "Epoch 320/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0879 - acc: 0.9603 - val_loss: 0.0975 - val_acc: 0.9538\n",
            "Epoch 321/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0869 - acc: 0.9628 - val_loss: 0.1034 - val_acc: 0.9590\n",
            "Epoch 322/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0873 - acc: 0.9615 - val_loss: 0.1009 - val_acc: 0.9590\n",
            "Epoch 323/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0854 - acc: 0.9667 - val_loss: 0.0957 - val_acc: 0.9538\n",
            "Epoch 324/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0882 - acc: 0.9641 - val_loss: 0.1006 - val_acc: 0.9590\n",
            "Epoch 325/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0896 - acc: 0.9590 - val_loss: 0.1106 - val_acc: 0.9487\n",
            "Epoch 326/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0883 - acc: 0.9577 - val_loss: 0.0966 - val_acc: 0.9590\n",
            "Epoch 327/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0858 - acc: 0.9667 - val_loss: 0.0965 - val_acc: 0.9590\n",
            "Epoch 328/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0849 - acc: 0.9692 - val_loss: 0.1032 - val_acc: 0.9538\n",
            "Epoch 329/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0862 - acc: 0.9590 - val_loss: 0.1008 - val_acc: 0.9590\n",
            "Epoch 330/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0840 - acc: 0.9692 - val_loss: 0.0940 - val_acc: 0.9538\n",
            "Epoch 331/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0862 - acc: 0.9679 - val_loss: 0.0978 - val_acc: 0.9590\n",
            "Epoch 332/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0847 - acc: 0.9641 - val_loss: 0.1057 - val_acc: 0.9487\n",
            "Epoch 333/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0849 - acc: 0.9577 - val_loss: 0.0964 - val_acc: 0.9590\n",
            "Epoch 334/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0860 - acc: 0.9679 - val_loss: 0.0936 - val_acc: 0.9538\n",
            "Epoch 335/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0842 - acc: 0.9692 - val_loss: 0.1046 - val_acc: 0.9487\n",
            "Epoch 336/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0873 - acc: 0.9564 - val_loss: 0.1001 - val_acc: 0.9590\n",
            "Epoch 337/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0830 - acc: 0.9641 - val_loss: 0.0917 - val_acc: 0.9641\n",
            "Epoch 338/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0873 - acc: 0.9654 - val_loss: 0.0961 - val_acc: 0.9590\n",
            "Epoch 339/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0828 - acc: 0.9628 - val_loss: 0.1049 - val_acc: 0.9487\n",
            "Epoch 340/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0841 - acc: 0.9603 - val_loss: 0.0968 - val_acc: 0.9590\n",
            "Epoch 341/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0816 - acc: 0.9705 - val_loss: 0.0934 - val_acc: 0.9590\n",
            "Epoch 342/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0828 - acc: 0.9692 - val_loss: 0.0969 - val_acc: 0.9590\n",
            "Epoch 343/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0812 - acc: 0.9692 - val_loss: 0.1053 - val_acc: 0.9487\n",
            "Epoch 344/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0833 - acc: 0.9590 - val_loss: 0.0971 - val_acc: 0.9590\n",
            "Epoch 345/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0831 - acc: 0.9705 - val_loss: 0.0930 - val_acc: 0.9590\n",
            "Epoch 346/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0816 - acc: 0.9692 - val_loss: 0.0987 - val_acc: 0.9538\n",
            "Epoch 347/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0808 - acc: 0.9679 - val_loss: 0.0957 - val_acc: 0.9590\n",
            "Epoch 348/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0801 - acc: 0.9705 - val_loss: 0.0936 - val_acc: 0.9590\n",
            "Epoch 349/2000\n",
            "780/780 [==============================] - 0s 22us/step - loss: 0.0799 - acc: 0.9705 - val_loss: 0.0928 - val_acc: 0.9590\n",
            "Epoch 350/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0798 - acc: 0.9718 - val_loss: 0.0950 - val_acc: 0.9538\n",
            "Epoch 351/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0794 - acc: 0.9705 - val_loss: 0.0967 - val_acc: 0.9538\n",
            "Epoch 352/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0795 - acc: 0.9679 - val_loss: 0.0960 - val_acc: 0.9538\n",
            "Epoch 353/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0790 - acc: 0.9705 - val_loss: 0.0950 - val_acc: 0.9538\n",
            "Epoch 354/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0788 - acc: 0.9705 - val_loss: 0.0938 - val_acc: 0.9538\n",
            "Epoch 355/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0789 - acc: 0.9718 - val_loss: 0.0925 - val_acc: 0.9590\n",
            "Epoch 356/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0784 - acc: 0.9731 - val_loss: 0.0909 - val_acc: 0.9590\n",
            "Epoch 357/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0787 - acc: 0.9692 - val_loss: 0.0930 - val_acc: 0.9538\n",
            "Epoch 358/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0782 - acc: 0.9718 - val_loss: 0.0954 - val_acc: 0.9538\n",
            "Epoch 359/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0785 - acc: 0.9705 - val_loss: 0.0934 - val_acc: 0.9590\n",
            "Epoch 360/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0775 - acc: 0.9718 - val_loss: 0.0946 - val_acc: 0.9538\n",
            "Epoch 361/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0775 - acc: 0.9705 - val_loss: 0.0913 - val_acc: 0.9590\n",
            "Epoch 362/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0772 - acc: 0.9705 - val_loss: 0.0895 - val_acc: 0.9641\n",
            "Epoch 363/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0773 - acc: 0.9705 - val_loss: 0.0931 - val_acc: 0.9590\n",
            "Epoch 364/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0767 - acc: 0.9718 - val_loss: 0.0945 - val_acc: 0.9538\n",
            "Epoch 365/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0768 - acc: 0.9692 - val_loss: 0.0913 - val_acc: 0.9590\n",
            "Epoch 366/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0763 - acc: 0.9731 - val_loss: 0.0901 - val_acc: 0.9590\n",
            "Epoch 367/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0763 - acc: 0.9692 - val_loss: 0.0896 - val_acc: 0.9590\n",
            "Epoch 368/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0758 - acc: 0.9731 - val_loss: 0.0937 - val_acc: 0.9538\n",
            "Epoch 369/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0760 - acc: 0.9705 - val_loss: 0.0902 - val_acc: 0.9590\n",
            "Epoch 370/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0751 - acc: 0.9718 - val_loss: 0.0869 - val_acc: 0.9641\n",
            "Epoch 371/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0760 - acc: 0.9692 - val_loss: 0.0895 - val_acc: 0.9590\n",
            "Epoch 372/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0747 - acc: 0.9744 - val_loss: 0.0947 - val_acc: 0.9538\n",
            "Epoch 373/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0758 - acc: 0.9679 - val_loss: 0.0910 - val_acc: 0.9538\n",
            "Epoch 374/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0746 - acc: 0.9718 - val_loss: 0.0856 - val_acc: 0.9641\n",
            "Epoch 375/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0753 - acc: 0.9705 - val_loss: 0.0894 - val_acc: 0.9590\n",
            "Epoch 376/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0739 - acc: 0.9744 - val_loss: 0.0942 - val_acc: 0.9538\n",
            "Epoch 377/2000\n",
            "780/780 [==============================] - 0s 22us/step - loss: 0.0749 - acc: 0.9679 - val_loss: 0.0916 - val_acc: 0.9538\n",
            "Epoch 378/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0734 - acc: 0.9731 - val_loss: 0.0853 - val_acc: 0.9692\n",
            "Epoch 379/2000\n",
            "780/780 [==============================] - 0s 25us/step - loss: 0.0747 - acc: 0.9705 - val_loss: 0.0865 - val_acc: 0.9641\n",
            "Epoch 380/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0731 - acc: 0.9705 - val_loss: 0.0928 - val_acc: 0.9538\n",
            "Epoch 381/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0743 - acc: 0.9692 - val_loss: 0.0922 - val_acc: 0.9538\n",
            "Epoch 382/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0734 - acc: 0.9705 - val_loss: 0.0848 - val_acc: 0.9692\n",
            "Epoch 383/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0739 - acc: 0.9718 - val_loss: 0.0883 - val_acc: 0.9641\n",
            "Epoch 384/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0732 - acc: 0.9705 - val_loss: 0.0954 - val_acc: 0.9538\n",
            "Epoch 385/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0737 - acc: 0.9705 - val_loss: 0.0856 - val_acc: 0.9641\n",
            "Epoch 386/2000\n",
            "780/780 [==============================] - 0s 25us/step - loss: 0.0724 - acc: 0.9705 - val_loss: 0.0841 - val_acc: 0.9641\n",
            "Epoch 387/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0720 - acc: 0.9731 - val_loss: 0.0915 - val_acc: 0.9538\n",
            "Epoch 388/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0722 - acc: 0.9731 - val_loss: 0.0940 - val_acc: 0.9538\n",
            "Epoch 389/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0726 - acc: 0.9692 - val_loss: 0.0862 - val_acc: 0.9641\n",
            "Epoch 390/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0732 - acc: 0.9718 - val_loss: 0.0838 - val_acc: 0.9744\n",
            "Epoch 391/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0723 - acc: 0.9756 - val_loss: 0.0918 - val_acc: 0.9538\n",
            "Epoch 392/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0723 - acc: 0.9718 - val_loss: 0.0885 - val_acc: 0.9590\n",
            "Epoch 393/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0712 - acc: 0.9769 - val_loss: 0.0855 - val_acc: 0.9641\n",
            "Epoch 394/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0721 - acc: 0.9718 - val_loss: 0.0836 - val_acc: 0.9641\n",
            "Epoch 395/2000\n",
            "780/780 [==============================] - 0s 25us/step - loss: 0.0699 - acc: 0.9756 - val_loss: 0.0928 - val_acc: 0.9538\n",
            "Epoch 396/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0722 - acc: 0.9679 - val_loss: 0.0883 - val_acc: 0.9538\n",
            "Epoch 397/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0705 - acc: 0.9744 - val_loss: 0.0812 - val_acc: 0.9795\n",
            "Epoch 398/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0720 - acc: 0.9692 - val_loss: 0.0860 - val_acc: 0.9590\n",
            "Epoch 399/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0698 - acc: 0.9756 - val_loss: 0.0876 - val_acc: 0.9590\n",
            "Epoch 400/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0694 - acc: 0.9744 - val_loss: 0.0831 - val_acc: 0.9641\n",
            "Epoch 401/2000\n",
            "780/780 [==============================] - 0s 24us/step - loss: 0.0693 - acc: 0.9731 - val_loss: 0.0849 - val_acc: 0.9590\n",
            "Epoch 402/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0696 - acc: 0.9756 - val_loss: 0.0875 - val_acc: 0.9590\n",
            "Epoch 403/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0685 - acc: 0.9769 - val_loss: 0.0812 - val_acc: 0.9795\n",
            "Epoch 404/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0714 - acc: 0.9705 - val_loss: 0.0823 - val_acc: 0.9641\n",
            "Epoch 405/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.0705 - acc: 0.9679 - val_loss: 0.0929 - val_acc: 0.9538\n",
            "Epoch 406/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0695 - acc: 0.9705 - val_loss: 0.0818 - val_acc: 0.9590\n",
            "Epoch 407/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0686 - acc: 0.9705 - val_loss: 0.0802 - val_acc: 0.9795\n",
            "Epoch 408/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0694 - acc: 0.9731 - val_loss: 0.0875 - val_acc: 0.9590\n",
            "Epoch 409/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0679 - acc: 0.9769 - val_loss: 0.0853 - val_acc: 0.9590\n",
            "Epoch 410/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0672 - acc: 0.9756 - val_loss: 0.0816 - val_acc: 0.9795\n",
            "Epoch 411/2000\n",
            "780/780 [==============================] - 0s 25us/step - loss: 0.0672 - acc: 0.9756 - val_loss: 0.0821 - val_acc: 0.9641\n",
            "Epoch 412/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0669 - acc: 0.9756 - val_loss: 0.0842 - val_acc: 0.9590\n",
            "Epoch 413/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0668 - acc: 0.9756 - val_loss: 0.0838 - val_acc: 0.9590\n",
            "Epoch 414/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0666 - acc: 0.9769 - val_loss: 0.0831 - val_acc: 0.9590\n",
            "Epoch 415/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0678 - acc: 0.9769 - val_loss: 0.0820 - val_acc: 0.9590\n",
            "Epoch 416/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0665 - acc: 0.9756 - val_loss: 0.0869 - val_acc: 0.9590\n",
            "Epoch 417/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0664 - acc: 0.9769 - val_loss: 0.0807 - val_acc: 0.9641\n",
            "Epoch 418/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0670 - acc: 0.9744 - val_loss: 0.0796 - val_acc: 0.9795\n",
            "Epoch 419/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0659 - acc: 0.9744 - val_loss: 0.0876 - val_acc: 0.9590\n",
            "Epoch 420/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0665 - acc: 0.9769 - val_loss: 0.0825 - val_acc: 0.9590\n",
            "Epoch 421/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0646 - acc: 0.9782 - val_loss: 0.0774 - val_acc: 0.9795\n",
            "Epoch 422/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0674 - acc: 0.9756 - val_loss: 0.0795 - val_acc: 0.9795\n",
            "Epoch 423/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0645 - acc: 0.9731 - val_loss: 0.0894 - val_acc: 0.9590\n",
            "Epoch 424/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0673 - acc: 0.9692 - val_loss: 0.0829 - val_acc: 0.9590\n",
            "Epoch 425/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0658 - acc: 0.9731 - val_loss: 0.0763 - val_acc: 0.9795\n",
            "Epoch 426/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0661 - acc: 0.9756 - val_loss: 0.0853 - val_acc: 0.9590\n",
            "Epoch 427/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0657 - acc: 0.9756 - val_loss: 0.0929 - val_acc: 0.9538\n",
            "Epoch 428/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0666 - acc: 0.9705 - val_loss: 0.0781 - val_acc: 0.9795\n",
            "Epoch 429/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0641 - acc: 0.9744 - val_loss: 0.0763 - val_acc: 0.9795\n",
            "Epoch 430/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0651 - acc: 0.9756 - val_loss: 0.0837 - val_acc: 0.9590\n",
            "Epoch 431/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0663 - acc: 0.9744 - val_loss: 0.0863 - val_acc: 0.9590\n",
            "Epoch 432/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0632 - acc: 0.9756 - val_loss: 0.0754 - val_acc: 0.9795\n",
            "Epoch 433/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0678 - acc: 0.9756 - val_loss: 0.0791 - val_acc: 0.9692\n",
            "Epoch 434/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0667 - acc: 0.9718 - val_loss: 0.0941 - val_acc: 0.9538\n",
            "Epoch 435/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0683 - acc: 0.9718 - val_loss: 0.0768 - val_acc: 0.9795\n",
            "Epoch 436/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0627 - acc: 0.9782 - val_loss: 0.0757 - val_acc: 0.9795\n",
            "Epoch 437/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0631 - acc: 0.9795 - val_loss: 0.0805 - val_acc: 0.9590\n",
            "Epoch 438/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0630 - acc: 0.9795 - val_loss: 0.0790 - val_acc: 0.9641\n",
            "Epoch 439/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0621 - acc: 0.9795 - val_loss: 0.0750 - val_acc: 0.9795\n",
            "Epoch 440/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0625 - acc: 0.9808 - val_loss: 0.0795 - val_acc: 0.9641\n",
            "Epoch 441/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0615 - acc: 0.9795 - val_loss: 0.0818 - val_acc: 0.9590\n",
            "Epoch 442/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0618 - acc: 0.9795 - val_loss: 0.0789 - val_acc: 0.9641\n",
            "Epoch 443/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0611 - acc: 0.9782 - val_loss: 0.0763 - val_acc: 0.9795\n",
            "Epoch 444/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0612 - acc: 0.9808 - val_loss: 0.0778 - val_acc: 0.9641\n",
            "Epoch 445/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0612 - acc: 0.9795 - val_loss: 0.0789 - val_acc: 0.9641\n",
            "Epoch 446/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0605 - acc: 0.9795 - val_loss: 0.0817 - val_acc: 0.9590\n",
            "Epoch 447/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0609 - acc: 0.9808 - val_loss: 0.0763 - val_acc: 0.9795\n",
            "Epoch 448/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0601 - acc: 0.9833 - val_loss: 0.0735 - val_acc: 0.9795\n",
            "Epoch 449/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0614 - acc: 0.9795 - val_loss: 0.0771 - val_acc: 0.9744\n",
            "Epoch 450/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0606 - acc: 0.9795 - val_loss: 0.0827 - val_acc: 0.9590\n",
            "Epoch 451/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0607 - acc: 0.9795 - val_loss: 0.0745 - val_acc: 0.9795\n",
            "Epoch 452/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0606 - acc: 0.9833 - val_loss: 0.0731 - val_acc: 0.9795\n",
            "Epoch 453/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0599 - acc: 0.9808 - val_loss: 0.0807 - val_acc: 0.9590\n",
            "Epoch 454/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0599 - acc: 0.9808 - val_loss: 0.0803 - val_acc: 0.9590\n",
            "Epoch 455/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0594 - acc: 0.9808 - val_loss: 0.0748 - val_acc: 0.9795\n",
            "Epoch 456/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0588 - acc: 0.9821 - val_loss: 0.0743 - val_acc: 0.9795\n",
            "Epoch 457/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0590 - acc: 0.9846 - val_loss: 0.0774 - val_acc: 0.9795\n",
            "Epoch 458/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0602 - acc: 0.9769 - val_loss: 0.0788 - val_acc: 0.9641\n",
            "Epoch 459/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0581 - acc: 0.9808 - val_loss: 0.0725 - val_acc: 0.9795\n",
            "Epoch 460/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0599 - acc: 0.9821 - val_loss: 0.0731 - val_acc: 0.9795\n",
            "Epoch 461/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0584 - acc: 0.9833 - val_loss: 0.0783 - val_acc: 0.9641\n",
            "Epoch 462/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0582 - acc: 0.9808 - val_loss: 0.0782 - val_acc: 0.9641\n",
            "Epoch 463/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0587 - acc: 0.9821 - val_loss: 0.0742 - val_acc: 0.9795\n",
            "Epoch 464/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0577 - acc: 0.9795 - val_loss: 0.0758 - val_acc: 0.9641\n",
            "Epoch 465/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0577 - acc: 0.9808 - val_loss: 0.0744 - val_acc: 0.9795\n",
            "Epoch 466/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0575 - acc: 0.9833 - val_loss: 0.0727 - val_acc: 0.9795\n",
            "Epoch 467/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.0572 - acc: 0.9859 - val_loss: 0.0764 - val_acc: 0.9744\n",
            "Epoch 468/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0573 - acc: 0.9808 - val_loss: 0.0747 - val_acc: 0.9795\n",
            "Epoch 469/2000\n",
            "780/780 [==============================] - 0s 25us/step - loss: 0.0568 - acc: 0.9846 - val_loss: 0.0723 - val_acc: 0.9795\n",
            "Epoch 470/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0568 - acc: 0.9846 - val_loss: 0.0758 - val_acc: 0.9795\n",
            "Epoch 471/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0564 - acc: 0.9821 - val_loss: 0.0774 - val_acc: 0.9641\n",
            "Epoch 472/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0563 - acc: 0.9808 - val_loss: 0.0724 - val_acc: 0.9795\n",
            "Epoch 473/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0564 - acc: 0.9821 - val_loss: 0.0721 - val_acc: 0.9795\n",
            "Epoch 474/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0579 - acc: 0.9859 - val_loss: 0.0738 - val_acc: 0.9795\n",
            "Epoch 475/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0555 - acc: 0.9846 - val_loss: 0.0708 - val_acc: 0.9795\n",
            "Epoch 476/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0568 - acc: 0.9833 - val_loss: 0.0723 - val_acc: 0.9795\n",
            "Epoch 477/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0569 - acc: 0.9833 - val_loss: 0.0750 - val_acc: 0.9795\n",
            "Epoch 478/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0558 - acc: 0.9872 - val_loss: 0.0733 - val_acc: 0.9795\n",
            "Epoch 479/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0549 - acc: 0.9833 - val_loss: 0.0763 - val_acc: 0.9641\n",
            "Epoch 480/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0556 - acc: 0.9808 - val_loss: 0.0735 - val_acc: 0.9744\n",
            "Epoch 481/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0564 - acc: 0.9846 - val_loss: 0.0716 - val_acc: 0.9795\n",
            "Epoch 482/2000\n",
            "780/780 [==============================] - 0s 22us/step - loss: 0.0542 - acc: 0.9833 - val_loss: 0.0797 - val_acc: 0.9590\n",
            "Epoch 483/2000\n",
            "780/780 [==============================] - 0s 26us/step - loss: 0.0559 - acc: 0.9821 - val_loss: 0.0763 - val_acc: 0.9641\n",
            "Epoch 484/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.0553 - acc: 0.9821 - val_loss: 0.0715 - val_acc: 0.9795\n",
            "Epoch 485/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0547 - acc: 0.9846 - val_loss: 0.0733 - val_acc: 0.9795\n",
            "Epoch 486/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0541 - acc: 0.9859 - val_loss: 0.0734 - val_acc: 0.9744\n",
            "Epoch 487/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0539 - acc: 0.9859 - val_loss: 0.0741 - val_acc: 0.9744\n",
            "Epoch 488/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0538 - acc: 0.9821 - val_loss: 0.0705 - val_acc: 0.9795\n",
            "Epoch 489/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0546 - acc: 0.9846 - val_loss: 0.0703 - val_acc: 0.9795\n",
            "Epoch 490/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0533 - acc: 0.9821 - val_loss: 0.0781 - val_acc: 0.9641\n",
            "Epoch 491/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0544 - acc: 0.9821 - val_loss: 0.0742 - val_acc: 0.9744\n",
            "Epoch 492/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0531 - acc: 0.9821 - val_loss: 0.0702 - val_acc: 0.9795\n",
            "Epoch 493/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0532 - acc: 0.9859 - val_loss: 0.0720 - val_acc: 0.9795\n",
            "Epoch 494/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0537 - acc: 0.9833 - val_loss: 0.0725 - val_acc: 0.9795\n",
            "Epoch 495/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0531 - acc: 0.9846 - val_loss: 0.0703 - val_acc: 0.9795\n",
            "Epoch 496/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0526 - acc: 0.9859 - val_loss: 0.0749 - val_acc: 0.9744\n",
            "Epoch 497/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0537 - acc: 0.9808 - val_loss: 0.0739 - val_acc: 0.9744\n",
            "Epoch 498/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0521 - acc: 0.9859 - val_loss: 0.0686 - val_acc: 0.9846\n",
            "Epoch 499/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0530 - acc: 0.9846 - val_loss: 0.0728 - val_acc: 0.9795\n",
            "Epoch 500/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0516 - acc: 0.9872 - val_loss: 0.0780 - val_acc: 0.9641\n",
            "Epoch 501/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0539 - acc: 0.9808 - val_loss: 0.0728 - val_acc: 0.9795\n",
            "Epoch 502/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0516 - acc: 0.9846 - val_loss: 0.0678 - val_acc: 0.9846\n",
            "Epoch 503/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0534 - acc: 0.9846 - val_loss: 0.0755 - val_acc: 0.9641\n",
            "Epoch 504/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0516 - acc: 0.9833 - val_loss: 0.0836 - val_acc: 0.9590\n",
            "Epoch 505/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0550 - acc: 0.9769 - val_loss: 0.0711 - val_acc: 0.9744\n",
            "Epoch 506/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0511 - acc: 0.9859 - val_loss: 0.0676 - val_acc: 0.9846\n",
            "Epoch 507/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0523 - acc: 0.9846 - val_loss: 0.0738 - val_acc: 0.9744\n",
            "Epoch 508/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0528 - acc: 0.9833 - val_loss: 0.0762 - val_acc: 0.9641\n",
            "Epoch 509/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0507 - acc: 0.9872 - val_loss: 0.0683 - val_acc: 0.9846\n",
            "Epoch 510/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0520 - acc: 0.9846 - val_loss: 0.0709 - val_acc: 0.9744\n",
            "Epoch 511/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0496 - acc: 0.9885 - val_loss: 0.0793 - val_acc: 0.9641\n",
            "Epoch 512/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0521 - acc: 0.9808 - val_loss: 0.0747 - val_acc: 0.9692\n",
            "Epoch 513/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0507 - acc: 0.9872 - val_loss: 0.0678 - val_acc: 0.9846\n",
            "Epoch 514/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0514 - acc: 0.9872 - val_loss: 0.0711 - val_acc: 0.9744\n",
            "Epoch 515/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0507 - acc: 0.9872 - val_loss: 0.0720 - val_acc: 0.9744\n",
            "Epoch 516/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0492 - acc: 0.9872 - val_loss: 0.0683 - val_acc: 0.9846\n",
            "Epoch 517/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0511 - acc: 0.9859 - val_loss: 0.0702 - val_acc: 0.9846\n",
            "Epoch 518/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0504 - acc: 0.9872 - val_loss: 0.0778 - val_acc: 0.9692\n",
            "Epoch 519/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0516 - acc: 0.9833 - val_loss: 0.0714 - val_acc: 0.9846\n",
            "Epoch 520/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0492 - acc: 0.9872 - val_loss: 0.0710 - val_acc: 0.9846\n",
            "Epoch 521/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0488 - acc: 0.9885 - val_loss: 0.0732 - val_acc: 0.9744\n",
            "Epoch 522/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0505 - acc: 0.9846 - val_loss: 0.0710 - val_acc: 0.9795\n",
            "Epoch 523/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0478 - acc: 0.9885 - val_loss: 0.0661 - val_acc: 0.9846\n",
            "Epoch 524/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0512 - acc: 0.9846 - val_loss: 0.0711 - val_acc: 0.9795\n",
            "Epoch 525/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0488 - acc: 0.9846 - val_loss: 0.0799 - val_acc: 0.9641\n",
            "Epoch 526/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0503 - acc: 0.9821 - val_loss: 0.0683 - val_acc: 0.9846\n",
            "Epoch 527/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0528 - acc: 0.9808 - val_loss: 0.0667 - val_acc: 0.9846\n",
            "Epoch 528/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0490 - acc: 0.9859 - val_loss: 0.0819 - val_acc: 0.9641\n",
            "Epoch 529/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0520 - acc: 0.9821 - val_loss: 0.0736 - val_acc: 0.9744\n",
            "Epoch 530/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0472 - acc: 0.9859 - val_loss: 0.0652 - val_acc: 0.9846\n",
            "Epoch 531/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0500 - acc: 0.9833 - val_loss: 0.0675 - val_acc: 0.9846\n",
            "Epoch 532/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0472 - acc: 0.9833 - val_loss: 0.0784 - val_acc: 0.9641\n",
            "Epoch 533/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0496 - acc: 0.9821 - val_loss: 0.0710 - val_acc: 0.9846\n",
            "Epoch 534/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0473 - acc: 0.9846 - val_loss: 0.0664 - val_acc: 0.9846\n",
            "Epoch 535/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0474 - acc: 0.9859 - val_loss: 0.0703 - val_acc: 0.9846\n",
            "Epoch 536/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0472 - acc: 0.9859 - val_loss: 0.0705 - val_acc: 0.9846\n",
            "Epoch 537/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0480 - acc: 0.9846 - val_loss: 0.0680 - val_acc: 0.9846\n",
            "Epoch 538/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0463 - acc: 0.9872 - val_loss: 0.0730 - val_acc: 0.9795\n",
            "Epoch 539/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0469 - acc: 0.9859 - val_loss: 0.0689 - val_acc: 0.9846\n",
            "Epoch 540/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0468 - acc: 0.9872 - val_loss: 0.0662 - val_acc: 0.9846\n",
            "Epoch 541/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0461 - acc: 0.9885 - val_loss: 0.0705 - val_acc: 0.9846\n",
            "Epoch 542/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0462 - acc: 0.9872 - val_loss: 0.0693 - val_acc: 0.9846\n",
            "Epoch 543/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0461 - acc: 0.9872 - val_loss: 0.0665 - val_acc: 0.9846\n",
            "Epoch 544/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0458 - acc: 0.9859 - val_loss: 0.0699 - val_acc: 0.9846\n",
            "Epoch 545/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0454 - acc: 0.9897 - val_loss: 0.0727 - val_acc: 0.9795\n",
            "Epoch 546/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0457 - acc: 0.9872 - val_loss: 0.0686 - val_acc: 0.9846\n",
            "Epoch 547/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0463 - acc: 0.9872 - val_loss: 0.0664 - val_acc: 0.9846\n",
            "Epoch 548/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0457 - acc: 0.9872 - val_loss: 0.0722 - val_acc: 0.9795\n",
            "Epoch 549/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0453 - acc: 0.9872 - val_loss: 0.0680 - val_acc: 0.9846\n",
            "Epoch 550/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0455 - acc: 0.9859 - val_loss: 0.0669 - val_acc: 0.9846\n",
            "Epoch 551/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0444 - acc: 0.9885 - val_loss: 0.0722 - val_acc: 0.9795\n",
            "Epoch 552/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0454 - acc: 0.9872 - val_loss: 0.0703 - val_acc: 0.9846\n",
            "Epoch 553/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0447 - acc: 0.9872 - val_loss: 0.0662 - val_acc: 0.9846\n",
            "Epoch 554/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0457 - acc: 0.9859 - val_loss: 0.0663 - val_acc: 0.9846\n",
            "Epoch 555/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0454 - acc: 0.9885 - val_loss: 0.0726 - val_acc: 0.9795\n",
            "Epoch 556/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0445 - acc: 0.9872 - val_loss: 0.0669 - val_acc: 0.9846\n",
            "Epoch 557/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0444 - acc: 0.9872 - val_loss: 0.0670 - val_acc: 0.9846\n",
            "Epoch 558/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0439 - acc: 0.9872 - val_loss: 0.0718 - val_acc: 0.9795\n",
            "Epoch 559/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0440 - acc: 0.9872 - val_loss: 0.0690 - val_acc: 0.9846\n",
            "Epoch 560/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0439 - acc: 0.9872 - val_loss: 0.0668 - val_acc: 0.9846\n",
            "Epoch 561/2000\n",
            "780/780 [==============================] - 0s 30us/step - loss: 0.0444 - acc: 0.9859 - val_loss: 0.0701 - val_acc: 0.9846\n",
            "Epoch 562/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0433 - acc: 0.9885 - val_loss: 0.0680 - val_acc: 0.9846\n",
            "Epoch 563/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0430 - acc: 0.9885 - val_loss: 0.0664 - val_acc: 0.9846\n",
            "Epoch 564/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0430 - acc: 0.9885 - val_loss: 0.0673 - val_acc: 0.9846\n",
            "Epoch 565/2000\n",
            "780/780 [==============================] - 0s 26us/step - loss: 0.0428 - acc: 0.9885 - val_loss: 0.0666 - val_acc: 0.9846\n",
            "Epoch 566/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0429 - acc: 0.9885 - val_loss: 0.0670 - val_acc: 0.9846\n",
            "Epoch 567/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0427 - acc: 0.9885 - val_loss: 0.0689 - val_acc: 0.9846\n",
            "Epoch 568/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0429 - acc: 0.9885 - val_loss: 0.0683 - val_acc: 0.9846\n",
            "Epoch 569/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0424 - acc: 0.9872 - val_loss: 0.0697 - val_acc: 0.9795\n",
            "Epoch 570/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0423 - acc: 0.9872 - val_loss: 0.0675 - val_acc: 0.9846\n",
            "Epoch 571/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0421 - acc: 0.9885 - val_loss: 0.0675 - val_acc: 0.9846\n",
            "Epoch 572/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0419 - acc: 0.9885 - val_loss: 0.0696 - val_acc: 0.9846\n",
            "Epoch 573/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0425 - acc: 0.9872 - val_loss: 0.0685 - val_acc: 0.9846\n",
            "Epoch 574/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0416 - acc: 0.9872 - val_loss: 0.0640 - val_acc: 0.9846\n",
            "Epoch 575/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0427 - acc: 0.9859 - val_loss: 0.0674 - val_acc: 0.9846\n",
            "Epoch 576/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0416 - acc: 0.9885 - val_loss: 0.0694 - val_acc: 0.9795\n",
            "Epoch 577/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0413 - acc: 0.9885 - val_loss: 0.0659 - val_acc: 0.9846\n",
            "Epoch 578/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0418 - acc: 0.9872 - val_loss: 0.0662 - val_acc: 0.9846\n",
            "Epoch 579/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0431 - acc: 0.9872 - val_loss: 0.0702 - val_acc: 0.9846\n",
            "Epoch 580/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0412 - acc: 0.9872 - val_loss: 0.0648 - val_acc: 0.9846\n",
            "Epoch 581/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0416 - acc: 0.9859 - val_loss: 0.0661 - val_acc: 0.9846\n",
            "Epoch 582/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0409 - acc: 0.9885 - val_loss: 0.0673 - val_acc: 0.9846\n",
            "Epoch 583/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0406 - acc: 0.9885 - val_loss: 0.0660 - val_acc: 0.9846\n",
            "Epoch 584/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0404 - acc: 0.9885 - val_loss: 0.0657 - val_acc: 0.9846\n",
            "Epoch 585/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.0405 - acc: 0.9885 - val_loss: 0.0674 - val_acc: 0.9846\n",
            "Epoch 586/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0406 - acc: 0.9885 - val_loss: 0.0671 - val_acc: 0.9846\n",
            "Epoch 587/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0401 - acc: 0.9885 - val_loss: 0.0681 - val_acc: 0.9846\n",
            "Epoch 588/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0400 - acc: 0.9885 - val_loss: 0.0663 - val_acc: 0.9846\n",
            "Epoch 589/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0399 - acc: 0.9885 - val_loss: 0.0653 - val_acc: 0.9846\n",
            "Epoch 590/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0401 - acc: 0.9859 - val_loss: 0.0671 - val_acc: 0.9846\n",
            "Epoch 591/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0397 - acc: 0.9872 - val_loss: 0.0704 - val_acc: 0.9795\n",
            "Epoch 592/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0400 - acc: 0.9872 - val_loss: 0.0658 - val_acc: 0.9846\n",
            "Epoch 593/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0394 - acc: 0.9885 - val_loss: 0.0649 - val_acc: 0.9846\n",
            "Epoch 594/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0397 - acc: 0.9872 - val_loss: 0.0674 - val_acc: 0.9846\n",
            "Epoch 595/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0399 - acc: 0.9885 - val_loss: 0.0676 - val_acc: 0.9846\n",
            "Epoch 596/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0397 - acc: 0.9859 - val_loss: 0.0649 - val_acc: 0.9846\n",
            "Epoch 597/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0406 - acc: 0.9872 - val_loss: 0.0682 - val_acc: 0.9846\n",
            "Epoch 598/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0386 - acc: 0.9885 - val_loss: 0.0641 - val_acc: 0.9846\n",
            "Epoch 599/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0392 - acc: 0.9859 - val_loss: 0.0646 - val_acc: 0.9846\n",
            "Epoch 600/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0393 - acc: 0.9885 - val_loss: 0.0667 - val_acc: 0.9846\n",
            "Epoch 601/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0389 - acc: 0.9885 - val_loss: 0.0657 - val_acc: 0.9846\n",
            "Epoch 602/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0383 - acc: 0.9885 - val_loss: 0.0684 - val_acc: 0.9795\n",
            "Epoch 603/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0385 - acc: 0.9885 - val_loss: 0.0661 - val_acc: 0.9846\n",
            "Epoch 604/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0380 - acc: 0.9885 - val_loss: 0.0643 - val_acc: 0.9846\n",
            "Epoch 605/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0385 - acc: 0.9872 - val_loss: 0.0658 - val_acc: 0.9846\n",
            "Epoch 606/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0383 - acc: 0.9885 - val_loss: 0.0675 - val_acc: 0.9795\n",
            "Epoch 607/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0382 - acc: 0.9885 - val_loss: 0.0641 - val_acc: 0.9846\n",
            "Epoch 608/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0394 - acc: 0.9859 - val_loss: 0.0645 - val_acc: 0.9846\n",
            "Epoch 609/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0378 - acc: 0.9885 - val_loss: 0.0737 - val_acc: 0.9744\n",
            "Epoch 610/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0394 - acc: 0.9885 - val_loss: 0.0656 - val_acc: 0.9846\n",
            "Epoch 611/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0379 - acc: 0.9885 - val_loss: 0.0631 - val_acc: 0.9846\n",
            "Epoch 612/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0382 - acc: 0.9885 - val_loss: 0.0710 - val_acc: 0.9795\n",
            "Epoch 613/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0394 - acc: 0.9885 - val_loss: 0.0712 - val_acc: 0.9795\n",
            "Epoch 614/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0382 - acc: 0.9872 - val_loss: 0.0628 - val_acc: 0.9846\n",
            "Epoch 615/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0388 - acc: 0.9872 - val_loss: 0.0671 - val_acc: 0.9846\n",
            "Epoch 616/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0371 - acc: 0.9885 - val_loss: 0.0729 - val_acc: 0.9744\n",
            "Epoch 617/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0383 - acc: 0.9872 - val_loss: 0.0638 - val_acc: 0.9846\n",
            "Epoch 618/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0369 - acc: 0.9885 - val_loss: 0.0633 - val_acc: 0.9846\n",
            "Epoch 619/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0370 - acc: 0.9872 - val_loss: 0.0685 - val_acc: 0.9846\n",
            "Epoch 620/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0368 - acc: 0.9885 - val_loss: 0.0693 - val_acc: 0.9846\n",
            "Epoch 621/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0368 - acc: 0.9872 - val_loss: 0.0643 - val_acc: 0.9846\n",
            "Epoch 622/2000\n",
            "780/780 [==============================] - 0s 29us/step - loss: 0.0377 - acc: 0.9872 - val_loss: 0.0641 - val_acc: 0.9846\n",
            "Epoch 623/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0361 - acc: 0.9872 - val_loss: 0.0716 - val_acc: 0.9795\n",
            "Epoch 624/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0383 - acc: 0.9872 - val_loss: 0.0646 - val_acc: 0.9846\n",
            "Epoch 625/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0361 - acc: 0.9885 - val_loss: 0.0618 - val_acc: 0.9846\n",
            "Epoch 626/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0394 - acc: 0.9872 - val_loss: 0.0701 - val_acc: 0.9795\n",
            "Epoch 627/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0370 - acc: 0.9872 - val_loss: 0.0682 - val_acc: 0.9846\n",
            "Epoch 628/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0352 - acc: 0.9897 - val_loss: 0.0625 - val_acc: 0.9846\n",
            "Epoch 629/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0383 - acc: 0.9885 - val_loss: 0.0634 - val_acc: 0.9846\n",
            "Epoch 630/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0354 - acc: 0.9897 - val_loss: 0.0707 - val_acc: 0.9795\n",
            "Epoch 631/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0367 - acc: 0.9872 - val_loss: 0.0696 - val_acc: 0.9795\n",
            "Epoch 632/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0357 - acc: 0.9872 - val_loss: 0.0627 - val_acc: 0.9846\n",
            "Epoch 633/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0362 - acc: 0.9872 - val_loss: 0.0647 - val_acc: 0.9846\n",
            "Epoch 634/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0353 - acc: 0.9885 - val_loss: 0.0701 - val_acc: 0.9795\n",
            "Epoch 635/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0357 - acc: 0.9910 - val_loss: 0.0641 - val_acc: 0.9846\n",
            "Epoch 636/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0364 - acc: 0.9897 - val_loss: 0.0642 - val_acc: 0.9846\n",
            "Epoch 637/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0356 - acc: 0.9897 - val_loss: 0.0710 - val_acc: 0.9795\n",
            "Epoch 638/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0360 - acc: 0.9897 - val_loss: 0.0648 - val_acc: 0.9846\n",
            "Epoch 639/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0361 - acc: 0.9885 - val_loss: 0.0675 - val_acc: 0.9846\n",
            "Epoch 640/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0345 - acc: 0.9885 - val_loss: 0.0745 - val_acc: 0.9744\n",
            "Epoch 641/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0368 - acc: 0.9872 - val_loss: 0.0657 - val_acc: 0.9846\n",
            "Epoch 642/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0350 - acc: 0.9910 - val_loss: 0.0627 - val_acc: 0.9846\n",
            "Epoch 643/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0348 - acc: 0.9872 - val_loss: 0.0681 - val_acc: 0.9846\n",
            "Epoch 644/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0351 - acc: 0.9885 - val_loss: 0.0658 - val_acc: 0.9846\n",
            "Epoch 645/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0346 - acc: 0.9910 - val_loss: 0.0605 - val_acc: 0.9846\n",
            "Epoch 646/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0350 - acc: 0.9872 - val_loss: 0.0653 - val_acc: 0.9846\n",
            "Epoch 647/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0343 - acc: 0.9897 - val_loss: 0.0678 - val_acc: 0.9795\n",
            "Epoch 648/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0351 - acc: 0.9897 - val_loss: 0.0634 - val_acc: 0.9846\n",
            "Epoch 649/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0340 - acc: 0.9897 - val_loss: 0.0668 - val_acc: 0.9846\n",
            "Epoch 650/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0337 - acc: 0.9910 - val_loss: 0.0711 - val_acc: 0.9795\n",
            "Epoch 651/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0350 - acc: 0.9885 - val_loss: 0.0673 - val_acc: 0.9795\n",
            "Epoch 652/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0334 - acc: 0.9910 - val_loss: 0.0636 - val_acc: 0.9846\n",
            "Epoch 653/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0337 - acc: 0.9885 - val_loss: 0.0642 - val_acc: 0.9846\n",
            "Epoch 654/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0331 - acc: 0.9910 - val_loss: 0.0686 - val_acc: 0.9795\n",
            "Epoch 655/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0338 - acc: 0.9897 - val_loss: 0.0637 - val_acc: 0.9846\n",
            "Epoch 656/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0328 - acc: 0.9897 - val_loss: 0.0613 - val_acc: 0.9846\n",
            "Epoch 657/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0339 - acc: 0.9897 - val_loss: 0.0649 - val_acc: 0.9846\n",
            "Epoch 658/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.0329 - acc: 0.9897 - val_loss: 0.0659 - val_acc: 0.9846\n",
            "Epoch 659/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0334 - acc: 0.9897 - val_loss: 0.0626 - val_acc: 0.9846\n",
            "Epoch 660/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0327 - acc: 0.9910 - val_loss: 0.0651 - val_acc: 0.9846\n",
            "Epoch 661/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0334 - acc: 0.9897 - val_loss: 0.0644 - val_acc: 0.9846\n",
            "Epoch 662/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0322 - acc: 0.9910 - val_loss: 0.0609 - val_acc: 0.9846\n",
            "Epoch 663/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0333 - acc: 0.9910 - val_loss: 0.0643 - val_acc: 0.9846\n",
            "Epoch 664/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0321 - acc: 0.9923 - val_loss: 0.0654 - val_acc: 0.9846\n",
            "Epoch 665/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0322 - acc: 0.9923 - val_loss: 0.0622 - val_acc: 0.9846\n",
            "Epoch 666/2000\n",
            "780/780 [==============================] - 0s 22us/step - loss: 0.0330 - acc: 0.9885 - val_loss: 0.0637 - val_acc: 0.9846\n",
            "Epoch 667/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0317 - acc: 0.9923 - val_loss: 0.0690 - val_acc: 0.9846\n",
            "Epoch 668/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0334 - acc: 0.9910 - val_loss: 0.0622 - val_acc: 0.9846\n",
            "Epoch 669/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.0318 - acc: 0.9910 - val_loss: 0.0600 - val_acc: 0.9846\n",
            "Epoch 670/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0319 - acc: 0.9910 - val_loss: 0.0631 - val_acc: 0.9846\n",
            "Epoch 671/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0317 - acc: 0.9910 - val_loss: 0.0615 - val_acc: 0.9846\n",
            "Epoch 672/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0317 - acc: 0.9897 - val_loss: 0.0600 - val_acc: 0.9846\n",
            "Epoch 673/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0310 - acc: 0.9910 - val_loss: 0.0626 - val_acc: 0.9846\n",
            "Epoch 674/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0309 - acc: 0.9923 - val_loss: 0.0599 - val_acc: 0.9846\n",
            "Epoch 675/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0303 - acc: 0.9923 - val_loss: 0.0591 - val_acc: 0.9846\n",
            "Epoch 676/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0304 - acc: 0.9910 - val_loss: 0.0583 - val_acc: 0.9846\n",
            "Epoch 677/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0288 - acc: 0.9923 - val_loss: 0.0546 - val_acc: 0.9846\n",
            "Epoch 678/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0320 - acc: 0.9872 - val_loss: 0.0638 - val_acc: 0.9846\n",
            "Epoch 679/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0334 - acc: 0.9897 - val_loss: 0.0703 - val_acc: 0.9846\n",
            "Epoch 680/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0350 - acc: 0.9872 - val_loss: 0.0530 - val_acc: 0.9846\n",
            "Epoch 681/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0336 - acc: 0.9897 - val_loss: 0.0584 - val_acc: 0.9846\n",
            "Epoch 682/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0283 - acc: 0.9923 - val_loss: 0.0617 - val_acc: 0.9846\n",
            "Epoch 683/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0285 - acc: 0.9923 - val_loss: 0.0540 - val_acc: 0.9846\n",
            "Epoch 684/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0277 - acc: 0.9923 - val_loss: 0.0552 - val_acc: 0.9846\n",
            "Epoch 685/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0268 - acc: 0.9923 - val_loss: 0.0597 - val_acc: 0.9846\n",
            "Epoch 686/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0276 - acc: 0.9923 - val_loss: 0.0572 - val_acc: 0.9846\n",
            "Epoch 687/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0275 - acc: 0.9910 - val_loss: 0.0540 - val_acc: 0.9846\n",
            "Epoch 688/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0275 - acc: 0.9897 - val_loss: 0.0566 - val_acc: 0.9846\n",
            "Epoch 689/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0268 - acc: 0.9923 - val_loss: 0.0544 - val_acc: 0.9846\n",
            "Epoch 690/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0263 - acc: 0.9897 - val_loss: 0.0573 - val_acc: 0.9846\n",
            "Epoch 691/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0262 - acc: 0.9923 - val_loss: 0.0572 - val_acc: 0.9846\n",
            "Epoch 692/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0261 - acc: 0.9897 - val_loss: 0.0552 - val_acc: 0.9846\n",
            "Epoch 693/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0264 - acc: 0.9910 - val_loss: 0.0559 - val_acc: 0.9846\n",
            "Epoch 694/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0261 - acc: 0.9910 - val_loss: 0.0572 - val_acc: 0.9846\n",
            "Epoch 695/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0257 - acc: 0.9910 - val_loss: 0.0552 - val_acc: 0.9846\n",
            "Epoch 696/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0257 - acc: 0.9910 - val_loss: 0.0559 - val_acc: 0.9846\n",
            "Epoch 697/2000\n",
            "780/780 [==============================] - 0s 24us/step - loss: 0.0259 - acc: 0.9910 - val_loss: 0.0571 - val_acc: 0.9846\n",
            "Epoch 698/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0260 - acc: 0.9923 - val_loss: 0.0554 - val_acc: 0.9846\n",
            "Epoch 699/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.0253 - acc: 0.9923 - val_loss: 0.0537 - val_acc: 0.9846\n",
            "Epoch 700/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0267 - acc: 0.9897 - val_loss: 0.0566 - val_acc: 0.9846\n",
            "Epoch 701/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0252 - acc: 0.9910 - val_loss: 0.0562 - val_acc: 0.9846\n",
            "Epoch 702/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0252 - acc: 0.9910 - val_loss: 0.0564 - val_acc: 0.9846\n",
            "Epoch 703/2000\n",
            "780/780 [==============================] - 0s 9us/step - loss: 0.0249 - acc: 0.9910 - val_loss: 0.0587 - val_acc: 0.9846\n",
            "Epoch 704/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0253 - acc: 0.9936 - val_loss: 0.0573 - val_acc: 0.9846\n",
            "Epoch 705/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0253 - acc: 0.9910 - val_loss: 0.0555 - val_acc: 0.9846\n",
            "Epoch 706/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0256 - acc: 0.9910 - val_loss: 0.0546 - val_acc: 0.9795\n",
            "Epoch 707/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0249 - acc: 0.9936 - val_loss: 0.0600 - val_acc: 0.9846\n",
            "Epoch 708/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0261 - acc: 0.9923 - val_loss: 0.0597 - val_acc: 0.9846\n",
            "Epoch 709/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0247 - acc: 0.9923 - val_loss: 0.0568 - val_acc: 0.9846\n",
            "Epoch 710/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0241 - acc: 0.9923 - val_loss: 0.0568 - val_acc: 0.9795\n",
            "Epoch 711/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0235 - acc: 0.9910 - val_loss: 0.0618 - val_acc: 0.9846\n",
            "Epoch 712/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.0592 - val_acc: 0.9846\n",
            "Epoch 713/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0229 - acc: 0.9923 - val_loss: 0.0587 - val_acc: 0.9846\n",
            "Epoch 714/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0236 - acc: 0.9923 - val_loss: 0.0603 - val_acc: 0.9846\n",
            "Epoch 715/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0222 - acc: 0.9936 - val_loss: 0.0596 - val_acc: 0.9795\n",
            "Epoch 716/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9910 - val_loss: 0.0643 - val_acc: 0.9846\n",
            "Epoch 717/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9923 - val_loss: 0.0615 - val_acc: 0.9846\n",
            "Epoch 718/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0216 - acc: 0.9936 - val_loss: 0.0601 - val_acc: 0.9795\n",
            "Epoch 719/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0237 - acc: 0.9949 - val_loss: 0.0694 - val_acc: 0.9795\n",
            "Epoch 720/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0253 - acc: 0.9936 - val_loss: 0.0619 - val_acc: 0.9846\n",
            "Epoch 721/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0214 - acc: 0.9949 - val_loss: 0.0619 - val_acc: 0.9795\n",
            "Epoch 722/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0229 - acc: 0.9936 - val_loss: 0.0715 - val_acc: 0.9795\n",
            "Epoch 723/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0257 - acc: 0.9936 - val_loss: 0.0640 - val_acc: 0.9846\n",
            "Epoch 724/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0243 - acc: 0.9949 - val_loss: 0.0635 - val_acc: 0.9846\n",
            "Epoch 725/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0207 - acc: 0.9923 - val_loss: 0.0722 - val_acc: 0.9795\n",
            "Epoch 726/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9936 - val_loss: 0.0609 - val_acc: 0.9795\n",
            "Epoch 727/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0244 - acc: 0.9949 - val_loss: 0.0621 - val_acc: 0.9846\n",
            "Epoch 728/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0231 - acc: 0.9936 - val_loss: 0.0685 - val_acc: 0.9846\n",
            "Epoch 729/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0223 - acc: 0.9949 - val_loss: 0.0616 - val_acc: 0.9795\n",
            "Epoch 730/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0234 - acc: 0.9923 - val_loss: 0.0632 - val_acc: 0.9795\n",
            "Epoch 731/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0210 - acc: 0.9910 - val_loss: 0.0648 - val_acc: 0.9846\n",
            "Epoch 732/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0208 - acc: 0.9936 - val_loss: 0.0639 - val_acc: 0.9846\n",
            "Epoch 733/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0659 - val_acc: 0.9846\n",
            "Epoch 734/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9936 - val_loss: 0.0633 - val_acc: 0.9846\n",
            "Epoch 735/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0204 - acc: 0.9949 - val_loss: 0.0623 - val_acc: 0.9795\n",
            "Epoch 736/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0228 - acc: 0.9910 - val_loss: 0.0639 - val_acc: 0.9846\n",
            "Epoch 737/2000\n",
            "780/780 [==============================] - 0s 11us/step - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0641 - val_acc: 0.9846\n",
            "Epoch 738/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0198 - acc: 0.9949 - val_loss: 0.0679 - val_acc: 0.9846\n",
            "Epoch 739/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0209 - acc: 0.9936 - val_loss: 0.0634 - val_acc: 0.9795\n",
            "Epoch 740/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0223 - acc: 0.9949 - val_loss: 0.0656 - val_acc: 0.9846\n",
            "Epoch 741/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0220 - acc: 0.9936 - val_loss: 0.0705 - val_acc: 0.9795\n",
            "Epoch 742/2000\n",
            "780/780 [==============================] - 0s 10us/step - loss: 0.0227 - acc: 0.9949 - val_loss: 0.0637 - val_acc: 0.9795\n",
            "Epoch 743/2000\n",
            "780/780 [==============================] - 0s 19us/step - loss: 0.0225 - acc: 0.9936 - val_loss: 0.0705 - val_acc: 0.9846\n",
            "Epoch 744/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.0216 - acc: 0.9936 - val_loss: 0.0668 - val_acc: 0.9846\n",
            "Epoch 745/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0191 - acc: 0.9949 - val_loss: 0.0634 - val_acc: 0.9795\n",
            "Epoch 746/2000\n",
            "780/780 [==============================] - 0s 20us/step - loss: 0.0225 - acc: 0.9949 - val_loss: 0.0690 - val_acc: 0.9846\n",
            "Epoch 747/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0210 - acc: 0.9936 - val_loss: 0.0703 - val_acc: 0.9795\n",
            "Epoch 748/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0220 - acc: 0.9923 - val_loss: 0.0638 - val_acc: 0.9795\n",
            "Epoch 749/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0198 - acc: 0.9923 - val_loss: 0.0658 - val_acc: 0.9846\n",
            "Epoch 750/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0650 - val_acc: 0.9846\n",
            "Epoch 751/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0191 - acc: 0.9949 - val_loss: 0.0638 - val_acc: 0.9846\n",
            "Epoch 752/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0192 - acc: 0.9949 - val_loss: 0.0657 - val_acc: 0.9846\n",
            "Epoch 753/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.0192 - acc: 0.9949 - val_loss: 0.0665 - val_acc: 0.9846\n",
            "Epoch 754/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9936 - val_loss: 0.0647 - val_acc: 0.9846\n",
            "Epoch 755/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0198 - acc: 0.9949 - val_loss: 0.0643 - val_acc: 0.9795\n",
            "Epoch 756/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0195 - acc: 0.9962 - val_loss: 0.0655 - val_acc: 0.9795\n",
            "Epoch 757/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0197 - acc: 0.9936 - val_loss: 0.0694 - val_acc: 0.9846\n",
            "Epoch 758/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0202 - acc: 0.9923 - val_loss: 0.0644 - val_acc: 0.9795\n",
            "Epoch 759/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9962 - val_loss: 0.0666 - val_acc: 0.9846\n",
            "Epoch 760/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0191 - acc: 0.9936 - val_loss: 0.0655 - val_acc: 0.9846\n",
            "Epoch 761/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9962 - val_loss: 0.0647 - val_acc: 0.9795\n",
            "Epoch 762/2000\n",
            "780/780 [==============================] - 0s 21us/step - loss: 0.0186 - acc: 0.9974 - val_loss: 0.0689 - val_acc: 0.9846\n",
            "Epoch 763/2000\n",
            "780/780 [==============================] - 0s 24us/step - loss: 0.0204 - acc: 0.9923 - val_loss: 0.0706 - val_acc: 0.9846\n",
            "Epoch 764/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0195 - acc: 0.9936 - val_loss: 0.0685 - val_acc: 0.9846\n",
            "Epoch 765/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9962 - val_loss: 0.0646 - val_acc: 0.9795\n",
            "Epoch 766/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0202 - acc: 0.9949 - val_loss: 0.0654 - val_acc: 0.9846\n",
            "Epoch 767/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0667 - val_acc: 0.9846\n",
            "Epoch 768/2000\n",
            "780/780 [==============================] - 0s 23us/step - loss: 0.0219 - acc: 0.9936 - val_loss: 0.0664 - val_acc: 0.9846\n",
            "Epoch 769/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0206 - acc: 0.9949 - val_loss: 0.0656 - val_acc: 0.9795\n",
            "Epoch 770/2000\n",
            "780/780 [==============================] - 0s 16us/step - loss: 0.0192 - acc: 0.9949 - val_loss: 0.0733 - val_acc: 0.9795\n",
            "Epoch 771/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9923 - val_loss: 0.0651 - val_acc: 0.9795\n",
            "Epoch 772/2000\n",
            "780/780 [==============================] - 0s 17us/step - loss: 0.0190 - acc: 0.9949 - val_loss: 0.0648 - val_acc: 0.9795\n",
            "Epoch 773/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9974 - val_loss: 0.0722 - val_acc: 0.9795\n",
            "Epoch 774/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9936 - val_loss: 0.0638 - val_acc: 0.9795\n",
            "Epoch 775/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0196 - acc: 0.9949 - val_loss: 0.0636 - val_acc: 0.9795\n",
            "Epoch 776/2000\n",
            "780/780 [==============================] - 0s 12us/step - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0728 - val_acc: 0.9795\n",
            "Epoch 777/2000\n",
            "780/780 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9936 - val_loss: 0.0653 - val_acc: 0.9795\n",
            "Epoch 778/2000\n",
            "780/780 [==============================] - 0s 13us/step - loss: 0.0242 - acc: 0.9910 - val_loss: 0.0692 - val_acc: 0.9846\n",
            "Epoch 779/2000\n",
            "780/780 [==============================] - 0s 18us/step - loss: 0.0226 - acc: 0.9949 - val_loss: 0.0809 - val_acc: 0.9795\n",
            "Epoch 780/2000\n",
            "780/780 [==============================] - 0s 15us/step - loss: 0.0234 - acc: 0.9910 - val_loss: 0.0659 - val_acc: 0.9795\n",
            "975/975 [==============================] - 0s 31us/step\n",
            "\n",
            " Accuracy: 0.9897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrR8v7r2MhRd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title 다중 분류"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuua3JvGQGDD",
        "colab_type": "text"
      },
      "source": [
        "# 다중 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQkg1gDYNOQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "df_pre = pd.read_csv('/gdrive/My Drive/Colab Notebooks/인공지능강의안/data/wine.csv', header=None)\n",
        "\n",
        "df = df_pre.sample(frac=1)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:,0:11]\n",
        "Y = dataset[:,11]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnHkDgAaMhKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "\n",
        "e = LabelEncoder()\n",
        "e.fit(Y)\n",
        "Y = e.transform(Y)\n",
        "Y_encoded = np_utils.to_categorical(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3LQzhPAMhHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30,  input_dim=11, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X,Y_encoded, epochs=50, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu1UhDyhR7b4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3f3da3e0-fdbd-48f5-94cb-c3f101136426"
      },
      "source": [
        "model.evaluate(X,Y_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6497/6497 [==============================] - 1s 113us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2739850489455076, 0.43650915807295676]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ9TwAomYjFG",
        "colab_type": "text"
      },
      "source": [
        "## 보스턴 집값 데이터셋을 이용한 선형 회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5TOR0SIZbA_",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 로드 및 훈련/테스트 셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXfq2D4FZLLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# delim_whitespace=True : 데이터 구분자가 빈 공백인 경우\n",
        "house = pd.read_csv(\"/gdrive/My Drive/Colab Notebooks/인공지능강의안/data/housing.csv\", delim_whitespace=True, header=None)\n",
        "\n",
        "dataset = house.values\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "\n",
        "# 테스트셋을 30% 사용\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDIZJOUyZkhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim=13, activation='relu'))\n",
        "model.add(Dense(6, activation='relu'))\n",
        "# 아무것도 안썼을 경우 기본 활성화함수로 항등함수가 설정(출력이 그대로 나옴)\n",
        "model.add(Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rdXhmXkZqyq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17b55382-7101-43d8-88a2-c310e73a4184"
      },
      "source": [
        "# 모델 컴파일 및 실행\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=10, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc707d301d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm8XDKJ4Z0OY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "c5038c56-e35c-43af-b798-7ecbcd6dac28"
      },
      "source": [
        "# predict() : 테스트 데이터 셋으로 출력을 예측\n",
        "# flatten() : 다차원 데이터를 1차원 데이터로 변경  \n",
        "Y_prediction = model.predict(X_test).flatten()\n",
        "\n",
        "for i in range(10):\n",
        "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(Y_test[i],  Y_prediction[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "실제가격: 22.600, 예상가격: 20.500\n",
            "실제가격: 50.000, 예상가격: 26.415\n",
            "실제가격: 23.000, 예상가격: 21.981\n",
            "실제가격: 8.300, 예상가격: 12.533\n",
            "실제가격: 21.200, 예상가격: 18.358\n",
            "실제가격: 19.900, 예상가격: 21.984\n",
            "실제가격: 20.600, 예상가격: 19.354\n",
            "실제가격: 18.700, 예상가격: 24.155\n",
            "실제가격: 16.100, 예상가격: 19.002\n",
            "실제가격: 18.600, 예상가격: 13.196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZILbEXPamFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "price_diff = []\n",
        "# 실제 가격과 예상 가격 차이 계산\n",
        "for i in range(10):\n",
        "    price_diff.append(np.abs(Y_test[i] - Y_prediction[i]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHyzTh9bbiuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "eb2a74d0-ad35-4448-ec08-74dc4c9d071d"
      },
      "source": [
        "price_diff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.0997978210449233,\n",
              " 23.58519172668457,\n",
              " 1.0188045501708984,\n",
              " 4.232618522644042,\n",
              " 2.8423397064208977,\n",
              " 2.084104156494142,\n",
              " 1.245980834960939,\n",
              " 5.454735565185548,\n",
              " 2.9024814605712876,\n",
              " 5.403548812866212]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bas3BfYCbu9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b7a1708-f5c2-4036-885f-51fbd780f505"
      },
      "source": [
        "np.average(price_diff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.086960315704346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hHVJDAXXfjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}